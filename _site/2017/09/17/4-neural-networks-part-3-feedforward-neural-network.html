<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>4. Neural Networks Part 3: Feedforward neural network</title>
  <link href="/fonts.css" rel="stylesheet" charset="utf-8">
  <link rel="stylesheet" href="/style.css">
  <!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Neural Networks Part 3: Feedforward neural network | Rakesh Malviya</title>
<meta property="og:title" content="Neural Networks Part 3: Feedforward neural network" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Required Learning: My previous posts on Neural networks Feedforward neural network is neural network architecture consisting of layers of artificial neurons such that: Each neuron in a layer is connected to all neurons of previous layer, i.e. its input is output of all neurons of previous layers. Last layer is called output layer and returns , predicted y Other layers are called hidden layers first hidden layerâ€™s input is such that is a vector of size . Note: Some people consider an imaginary input layer of size which connects to first hidden layer. If you havenâ€™t read this anywhere just forget this note. For others I find this way of representing layers easier when will be implementing classes for different types of neural network layers. We can have zero to any number of hidden layers. In case of zero hidden layers our network for binary classification task will be same as logistic regression descrsibed in previous post. Think â€¦ ðŸ’­ Since we have only single artificial neuron of output layer in the whole neural network. Fig 1 : Feedforward neural network Establish Notations: For notational convenience we have removed superscript (i) for training example . So is single training input, is expected output and is predicted output Input is such that is a vector of size . We have hidden layers. Output layer is denoted by or Each layer has following attributes weight matrix , output vector and bias vector $$b^l$. I am proposing below notations for your easy of understanding the equations in upcoming sections. Also check Fig 1 above let output layer has size i.e artificial neurons. Layer has size and Layer has size We will use to denote individual neurons in layer , for layer and for output layer . You need to think carefully why I am proposing below sizes for weight matrix, output vector and bias. It will help if you assume this, read understand whole article, comeback to this and think. For layer weight matrix has size and for ouput layer has size For layer vectors and has size and for layer vectors and has size and for output layer vectors and has size Artificial neuron in layer has bias (scalar), output (scalar), weight vector , i.e. row of weight matrix for layer l. Also weight vector . It will help it you remember from my previous post on logistic reggression that each neuron has weight vector, (scalar or single value) bias and output. Below notations are neccessary to easy of understand and implementing mathematical equations in upcoming sections We will use for elementwise multiplication of two vectors, for example We will use for matrix multiplication Training Let us use our Neural network (in Fig 1) for classification. Note, for binary classification our last layer will have only one neuron hence , but we will keep our approach general. Also for simplicity we will assume that activation function of all the neurons is sigmoid function . Let our loss function, . So our goal is to minimize loss, . We can do this only by learning correct weights and bias for each neuron in our network. It looks complex if we look at each neuron individualy. We can simplify this task by breaking our training steps into three steps of Backpropagation: Forward pass: calculate output of each neuron. Backward pass: calculate and for each neuron. Update weights and biases Foward pass: Output of neuron in layer is defined as, We can use equation to get output of each neuron in forward pass Let total input to above neuron defined as, hence, similarly for output layer, Also, output of output layer is predicted hence, Forward pass (vectorized) It is important that we implement code in the vectors and matrix operations to improve preformance. Note: for vectors is called elementwise Useful derivatives: where, is derivative of with repect Similarly for loss , Backward pass Gradient of loss with respect to weights for output layer is, Let us define gradient of loss with respect to total input for output layer as, Similarly for layer , Similarly you can easily prove for bias, Backword pass (Vectorized) where is transpose of matrix and , Derivative of J with respect recpect to vector , i.e. Also, Where, , Derivative of J with respect recpect to matrix , i.e. Update step: Given a training batch of size and learning rate For each training example in in batch do Forward pass and Backward pass, accumulate and Update weights and bias for each layer as follows, td;dr. Code Here is the python implementation of the above article. References: Neural Networks and Deep Learning Chapter 2 link" />
<meta property="og:description" content="Required Learning: My previous posts on Neural networks Feedforward neural network is neural network architecture consisting of layers of artificial neurons such that: Each neuron in a layer is connected to all neurons of previous layer, i.e. its input is output of all neurons of previous layers. Last layer is called output layer and returns , predicted y Other layers are called hidden layers first hidden layerâ€™s input is such that is a vector of size . Note: Some people consider an imaginary input layer of size which connects to first hidden layer. If you havenâ€™t read this anywhere just forget this note. For others I find this way of representing layers easier when will be implementing classes for different types of neural network layers. We can have zero to any number of hidden layers. In case of zero hidden layers our network for binary classification task will be same as logistic regression descrsibed in previous post. Think â€¦ ðŸ’­ Since we have only single artificial neuron of output layer in the whole neural network. Fig 1 : Feedforward neural network Establish Notations: For notational convenience we have removed superscript (i) for training example . So is single training input, is expected output and is predicted output Input is such that is a vector of size . We have hidden layers. Output layer is denoted by or Each layer has following attributes weight matrix , output vector and bias vector $$b^l$. I am proposing below notations for your easy of understanding the equations in upcoming sections. Also check Fig 1 above let output layer has size i.e artificial neurons. Layer has size and Layer has size We will use to denote individual neurons in layer , for layer and for output layer . You need to think carefully why I am proposing below sizes for weight matrix, output vector and bias. It will help if you assume this, read understand whole article, comeback to this and think. For layer weight matrix has size and for ouput layer has size For layer vectors and has size and for layer vectors and has size and for output layer vectors and has size Artificial neuron in layer has bias (scalar), output (scalar), weight vector , i.e. row of weight matrix for layer l. Also weight vector . It will help it you remember from my previous post on logistic reggression that each neuron has weight vector, (scalar or single value) bias and output. Below notations are neccessary to easy of understand and implementing mathematical equations in upcoming sections We will use for elementwise multiplication of two vectors, for example We will use for matrix multiplication Training Let us use our Neural network (in Fig 1) for classification. Note, for binary classification our last layer will have only one neuron hence , but we will keep our approach general. Also for simplicity we will assume that activation function of all the neurons is sigmoid function . Let our loss function, . So our goal is to minimize loss, . We can do this only by learning correct weights and bias for each neuron in our network. It looks complex if we look at each neuron individualy. We can simplify this task by breaking our training steps into three steps of Backpropagation: Forward pass: calculate output of each neuron. Backward pass: calculate and for each neuron. Update weights and biases Foward pass: Output of neuron in layer is defined as, We can use equation to get output of each neuron in forward pass Let total input to above neuron defined as, hence, similarly for output layer, Also, output of output layer is predicted hence, Forward pass (vectorized) It is important that we implement code in the vectors and matrix operations to improve preformance. Note: for vectors is called elementwise Useful derivatives: where, is derivative of with repect Similarly for loss , Backward pass Gradient of loss with respect to weights for output layer is, Let us define gradient of loss with respect to total input for output layer as, Similarly for layer , Similarly you can easily prove for bias, Backword pass (Vectorized) where is transpose of matrix and , Derivative of J with respect recpect to vector , i.e. Also, Where, , Derivative of J with respect recpect to matrix , i.e. Update step: Given a training batch of size and learning rate For each training example in in batch do Forward pass and Backward pass, accumulate and Update weights and bias for each layer as follows, td;dr. Code Here is the python implementation of the above article. References: Neural Networks and Deep Learning Chapter 2 link" />
<link rel="canonical" href="http://localhost:4000/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html" />
<meta property="og:url" content="http://localhost:4000/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html" />
<meta property="og:site_name" content="Rakesh Malviya" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-09-17T00:00:00+05:30" />
<script type="application/ld+json">
{"name":null,"description":"Required Learning: My previous posts on Neural networks Feedforward neural network is neural network architecture consisting of layers of artificial neurons such that: Each neuron in a layer is connected to all neurons of previous layer, i.e. its input is output of all neurons of previous layers. Last layer is called output layer and returns , predicted y Other layers are called hidden layers first hidden layerâ€™s input is such that is a vector of size . Note: Some people consider an imaginary input layer of size which connects to first hidden layer. If you havenâ€™t read this anywhere just forget this note. For others I find this way of representing layers easier when will be implementing classes for different types of neural network layers. We can have zero to any number of hidden layers. In case of zero hidden layers our network for binary classification task will be same as logistic regression descrsibed in previous post. Think â€¦ ðŸ’­ Since we have only single artificial neuron of output layer in the whole neural network. Fig 1 : Feedforward neural network Establish Notations: For notational convenience we have removed superscript (i) for training example . So is single training input, is expected output and is predicted output Input is such that is a vector of size . We have hidden layers. Output layer is denoted by or Each layer has following attributes weight matrix , output vector and bias vector $$b^l$. I am proposing below notations for your easy of understanding the equations in upcoming sections. Also check Fig 1 above let output layer has size i.e artificial neurons. Layer has size and Layer has size We will use to denote individual neurons in layer , for layer and for output layer . You need to think carefully why I am proposing below sizes for weight matrix, output vector and bias. It will help if you assume this, read understand whole article, comeback to this and think. For layer weight matrix has size and for ouput layer has size For layer vectors and has size and for layer vectors and has size and for output layer vectors and has size Artificial neuron in layer has bias (scalar), output (scalar), weight vector , i.e. row of weight matrix for layer l. Also weight vector . It will help it you remember from my previous post on logistic reggression that each neuron has weight vector, (scalar or single value) bias and output. Below notations are neccessary to easy of understand and implementing mathematical equations in upcoming sections We will use for elementwise multiplication of two vectors, for example We will use for matrix multiplication Training Let us use our Neural network (in Fig 1) for classification. Note, for binary classification our last layer will have only one neuron hence , but we will keep our approach general. Also for simplicity we will assume that activation function of all the neurons is sigmoid function . Let our loss function, . So our goal is to minimize loss, . We can do this only by learning correct weights and bias for each neuron in our network. It looks complex if we look at each neuron individualy. We can simplify this task by breaking our training steps into three steps of Backpropagation: Forward pass: calculate output of each neuron. Backward pass: calculate and for each neuron. Update weights and biases Foward pass: Output of neuron in layer is defined as, We can use equation to get output of each neuron in forward pass Let total input to above neuron defined as, hence, similarly for output layer, Also, output of output layer is predicted hence, Forward pass (vectorized) It is important that we implement code in the vectors and matrix operations to improve preformance. Note: for vectors is called elementwise Useful derivatives: where, is derivative of with repect Similarly for loss , Backward pass Gradient of loss with respect to weights for output layer is, Let us define gradient of loss with respect to total input for output layer as, Similarly for layer , Similarly you can easily prove for bias, Backword pass (Vectorized) where is transpose of matrix and , Derivative of J with respect recpect to vector , i.e. Also, Where, , Derivative of J with respect recpect to matrix , i.e. Update step: Given a training batch of size and learning rate For each training example in in batch do Forward pass and Backward pass, accumulate and Update weights and bias for each layer as follows, td;dr. Code Here is the python implementation of the above article. References: Neural Networks and Deep Learning Chapter 2 link","author":null,"@type":"BlogPosting","url":"http://localhost:4000/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html","image":null,"publisher":null,"headline":"Neural Networks Part 3: Feedforward neural network","dateModified":"2017-09-17T00:00:00+05:30","datePublished":"2017-09-17T00:00:00+05:30","sameAs":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>
<body>
  <div class="container_post">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<header class="masthead">
  <h1 class="masthead-title--small">
    <a href="/">Rakesh Malviya</a>
  </h1>
</header>      
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v2.10";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>
        <div class="fb-share-button" data-href="/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html" data-layout="button_count" data-size="small" data-mobile-iframe="true"><a class="fb-xfbml-parse-ignore" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdevelopers.facebook.com%2Fdocs%2Fplugins%2F&amp;src=sdkpreparse">Share</a></div>

        <a href="https://twitter.com/share" class="twitter-share-button" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

        <!-- Place this tag in your head or just before your close body tag. -->
        <script src="https://apis.google.com/js/platform.js" async defer></script>
        <!-- Place this tag where you want the share button to render. -->
        <div class="g-plus" data-action="share"></div>
        <hr/>

<div class="content post">
  <h1 class="post-title">4. Neural Networks Part 3: Feedforward neural network</h1>
  <div class="meta_wrapper">
  <span class="post-date">17 Sep 2017</span>
  
  
    <a href="/tags#neural-networks" class="post-tag">Neural Networks</a>
  
    <a href="/tags#machine-learning" class="post-tag">Machine Learning</a>
  
    <a href="/tags#deep-learning" class="post-tag">Deep Learning</a>
  
  
<div class="meta_wrapper">  
  <p><strong>Required Learning:</strong> My previous posts on Neural networks</p>

<p><strong>Feedforward neural network</strong> is neural network architecture consisting of layers of artificial neurons such that:</p>
<ol>
  <li>Each neuron in a layer is connected to all neurons of previous layer, i.e. its input is output of all neurons of previous layers.</li>
  <li>Last layer is called output layer and returns <script type="math/tex">y'</script>, predicted y</li>
  <li>Other layers are called hidden layers</li>
  <li>first hidden layerâ€™s input is <script type="math/tex">x</script> such that <script type="math/tex">x</script> is a vector <script type="math/tex">(x_1,x_2,x_3,..... x_n)</script> of size <script type="math/tex">n</script>. <strong>Note:</strong> Some people consider an imaginary input layer of size <script type="math/tex">n</script> which connects to first hidden layer. If you havenâ€™t read this anywhere just forget this note. For others I find this way of representing layers easier when will be implementing classes for different types of neural network layers.</li>
  <li>We can have zero to any number of hidden layers. In case of zero hidden layers our network for binary classification task will be same as logistic regression descrsibed in previous post. Think â€¦ ðŸ’­ Since we have only single artificial neuron of output layer in the whole neural network.</li>
</ol>

<p><img src="/assets/img/blog/4/4_1_neuralnetwork.svg" alt="" title="Feedforward neural network" />
<i><center>Fig 1 : Feedforward neural network</center></i></p>

<h4 id="establish-notations">Establish Notations:</h4>
<ol>
  <li>For notational convenience we have removed superscript (i) for <script type="math/tex">i^{th}</script> training example <script type="math/tex">(x,y)</script>. So <script type="math/tex">x</script> is single training input, <script type="math/tex">y</script> is expected output and <script type="math/tex">y'</script> is predicted output</li>
  <li>Input <script type="math/tex">x</script> is such that <script type="math/tex">x</script> is a vector <script type="math/tex">(x_1,x_2,x_3,..... x_n)</script> of size <script type="math/tex">n</script>.</li>
  <li>We have <script type="math/tex">l</script> hidden layers. Output layer is denoted by <script type="math/tex">l+1</script> or <script type="math/tex">o</script></li>
  <li>Each layer <script type="math/tex">l</script> has following attributes weight matrix <script type="math/tex">W^l</script>, output vector <script type="math/tex">h^l</script> and bias vector $$b^l$.</li>
  <li><strong>I am proposing below notations for your easy of understanding the equations in upcoming sections. Also check Fig 1 above</strong></li>
  <li>let output layer has size <script type="math/tex">r</script> i.e <script type="math/tex">r</script> artificial neurons. Layer <script type="math/tex">l</script> has size <script type="math/tex">q</script> and Layer <script type="math/tex">l-1</script> has size <script type="math/tex">p</script></li>
  <li>We will use <script type="math/tex">i</script> to denote individual neurons in layer <script type="math/tex">l-1</script>, <script type="math/tex">j</script> for layer <script type="math/tex">l</script> and <script type="math/tex">k</script> for output layer <script type="math/tex">o</script>.</li>
  <li><strong>You need to think carefully why I am proposing below sizes for weight matrix, output vector and bias. It will help if you assume this, read understand whole article, comeback to this and think</strong>.</li>
  <li>For layer  <script type="math/tex">l</script> weight matrix <script type="math/tex">W^l</script> has size <script type="math/tex">[qxp]</script> and for ouput layer <script type="math/tex">W^o</script> has size <script type="math/tex">[rxq]</script></li>
  <li>For layer  <script type="math/tex">l-1</script> vectors <script type="math/tex">h^{l-1}</script> and <script type="math/tex">b^{l-1}</script> has size <script type="math/tex">p</script> and  for layer  <script type="math/tex">l</script> vectors <script type="math/tex">h^l</script> and <script type="math/tex">b^l</script> has size <script type="math/tex">q</script> and for output layer <script type="math/tex">o</script> vectors <script type="math/tex">h^o</script> and <script type="math/tex">b^o</script> has size <script type="math/tex">r</script></li>
  <li><script type="math/tex">j^{th}</script> Artificial neuron in layer <script type="math/tex">l</script>  has bias <script type="math/tex">b_j^l</script> (scalar), output <script type="math/tex">h_j^l</script>(scalar),  weight vector <script type="math/tex">W_{j:}^l</script> , i.e. <script type="math/tex">j^{th}</script> row of weight matrix <script type="math/tex">W^l</script> for layer l. Also weight vector <script type="math/tex">W_{j:}^l = (w_{j1},w_{j2},....w_{ji}...,w_{jp},)</script>. <em>It will help it you remember from my previous post on logistic reggression that each neuron has weight vector, (scalar or single value) bias and output</em>.</li>
  <li><strong>Below notations are neccessary to easy of understand and implementing mathematical equations in upcoming sections</strong></li>
  <li>We will use <script type="math/tex">\odot</script> for elementwise multiplication of two vectors, for example <script type="math/tex">\left[\begin{array}{c} 1 \\ 2 \end{array}\right]   \odot \left[\begin{array}{c} 1 \\ 2\end{array} \right]= \left[ \begin{array}{c} {1 \cdot 1} \\ {2 \cdot 2} \end{array} \right]= \left[ \begin{array}{c} 1 \\ 4 \end{array} \right]</script></li>
  <li>We will use <script type="math/tex">\otimes</script> for matrix multiplication</li>
</ol>

<h4 id="training">Training</h4>
<p>Let us use our Neural network (in Fig 1) for classification. Note, for binary classification our last layer will have only one neuron hence <script type="math/tex">r==1</script>, but we will keep our approach general. Also for simplicity we will assume that activation function of all the neurons is sigmoid function <script type="math/tex">\sigma()</script>.</p>

<p>Let our loss function, <script type="math/tex">J = \frac{1}{2}\sum_{k=1}^{r}(y_k - y'_k)^2</script>.</p>

<p>So our goal is to minimize loss, <script type="math/tex">J</script>. We can do this only by learning correct weights and bias for each neuron in our network. It looks complex if we look at each neuron individualy. We can simplify this task by breaking our training steps into three steps of <strong>Backpropagation</strong>:</p>
<ol>
  <li><strong>Forward pass:</strong> calculate output of each neuron.</li>
  <li><strong>Backward pass:</strong> calculate <script type="math/tex">\frac{\partial{J}}{\partial{b_j^l}}</script> and <script type="math/tex">\frac{\partial{J}}{\partial{w_{ji}^l}}</script> for each neuron.</li>
  <li><strong>Update weights and biases</strong></li>
</ol>

<h4 id="foward-pass">Foward pass:</h4>
<p>Output of <script type="math/tex">j^{th}</script> neuron in <script type="math/tex">l^{th}</script> layer is defined as, 
<script type="math/tex">\begin{align} 
h_j^l = \sigma(\sum_{i=1}^{p} w_{ji}^l  h_i^{l-1} + b_j^l) \tag{1} \label{eq1}
\end{align}</script></p>

<p><strong>We can use equation <script type="math/tex">\eqref{eq1}</script> to get output of each neuron in forward pass</strong></p>

<p>Let total input to above neuron defined as, 
<script type="math/tex">\begin{align} 
z_j^l = \sum_{i=1}^{p} w_{ji}^l  h_i^{l-1} + b_j^l \tag{2} \label{eq2}
\end{align}</script></p>

<p>hence, 
<script type="math/tex">% <![CDATA[
\begin{align} 
h_j^l &= \sigma(z_j^l) && \text{by \eqref{eq1} and \eqref{eq2}} \tag{3} \label{eq3}
\end{align} %]]></script></p>

<p>similarly for output layer, 
<script type="math/tex">% <![CDATA[
\begin{align} 
h_k^o &= \sigma(z_k^o) && \text{by \eqref{eq3}} \tag{4} \label{eq4}\\
z_k^o &= \sum_{j=1}^{q} w_{kj}^o  h_j^{l} + b_k^o && \text{by \eqref{eq2}} \tag{5} \label{eq5}
\end{align} %]]></script></p>

<p>Also, output of output layer is predicted <script type="math/tex">y</script> hence,</p>

<script type="math/tex; mode=display">\begin{align} 
h_k^o = y'_k \tag{6} \label{eq6}
\end{align}</script>

<h6 id="forward-pass-vectorized">Forward pass (vectorized)</h6>
<p>It is important that we implement code in the vectors and matrix operations to improve preformance.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
h^l &= \sigma(z^l) && \text{by \eqref{eq3}} \tag{7} \label{eq7}\\
z^l &= W^l \otimes h^{l-1} + b^l && \text{by \eqref{eq2}}  \tag{8} \label{eq8}\\
h^o &= \sigma(z^o) && \text{by \eqref{eq4}} \tag{9} \label{eq9}\\
z^o &= W^o \otimes h^{l} + b^o && \text{by \eqref{eq5}}  \tag{10} \label{eq10}\\
\end{align} %]]></script>

<p><strong>Note: for vectors <script type="math/tex">\sigma()</script> is called elementwise</strong></p>

<h6 id="useful-derivatives">Useful derivatives:</h6>
<p><script type="math/tex">% <![CDATA[
\begin{align} 
\frac{\partial{h_j^l}}{\partial z_j^l} &= \frac{\partial}{\partial z_j^l}{\sigma({z_j^l}}) && \text{by \eqref{eq3}} \\
\frac{\partial{h_j^l}}{\partial z_j^l} &= \sigma'({z_j^l}) && \tag{11} \label{eq11} \\
\frac{\partial{z_j^l}}{\partial w_{ji}^l} &= h_i^{l-1} && \tag{12} \label{eq12}
\end{align} %]]></script></p>

<p>where, <script type="math/tex">\sigma'(z)</script> is derivative of <script type="math/tex">\sigma(z)</script> with repect <script type="math/tex">z</script></p>

<p>Similarly for loss <script type="math/tex">J</script>,
<script type="math/tex">% <![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial y'_k} &= (y'_k - y_k) && \text{how ? check prev post} \tag{13} \label{eq13}
\end{align} %]]></script></p>

<h4 id="backward-pass">Backward pass</h4>

<p>Gradient of loss with respect to weights for output layer is,<br />
<script type="math/tex">% <![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{w_{kj}^o}} &= \frac{\partial{J}}{\partial{y'_k}} \cdot \frac{\partial{y'_k}}{\partial{z_k^o}} \cdot \frac{\partial{z_k^o}}{\partial{w_{kj}^o}}  &&\text{by chain rule} \tag{14} \label{eq14} \\
\frac{\partial{J}}{\partial{w_{kj}^o}} &=(y'_k - y_k)  \cdot  \sigma'({z_k^o}) \cdot h_j^l  &&\text{by \eqref{eq11},\eqref{eq12},\eqref{eq13}}  \tag{15} \label{eq15}\\
\end{align} %]]></script></p>

<p>Let us define gradient of loss with respect to total input for output layer as,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
\delta_k^o &= \frac{\partial{J}}{\partial{z_k^o}} \tag{16} \label{eq16}\\
&= \frac{\partial{J}}{\partial{y'_k}} \cdot \frac{\partial{y'_k}}{\partial{z_k^o}} \\
\delta_k^o &= \frac{\partial{J}}{\partial{y'_k}} \cdot \sigma'({z_k^o}) \tag{17} \label{eq17}\\
\frac{\partial{J}}{\partial{w_{kj}^o}} &=\delta_k^o \cdot h_j^l  &&\text{by \eqref{eq15},\eqref{eq17}} \tag{18} \label{eq18}\\
\end{align} %]]></script>

<p>Similarly for layer <script type="math/tex">l</script>,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{w_{ji}^l}} &=\delta_j^l \cdot h_i^{l-1} \tag{19} \label{eq19} \\
\delta_j^l &= \frac{\partial{J}}{\partial{z_j^l}} \\
&= \sum_{k=1}^{r} \frac{\partial{J}}{\partial{z_k^o}} \cdot  \frac{\partial{z_k^o}}{\partial{z_j^l}}\\
&= \sum_{k=1}^{r} \frac{\partial{J}}{\partial{z_k^o}} \cdot  \frac{\partial}{\partial{z_j^l}} \big( \sum_{j=1}^{q} w_{kj}^o  h_j^{l} + b_k^o \big) && \text{by \eqref{eq5}}\\
&= \sum_{k=1}^{r} \frac{\partial{J}}{\partial{z_k^o}} \cdot w_{kj}^o \cdot  \frac{\partial{h_j^l}}{\partial{z_j^l}} \\
\delta_j^l &= \sum_{k=1}^{r} \delta_k^o \cdot w_{kj}^o \cdot \sigma'({z_j^l}) && \text{by \eqref{eq11},\eqref{eq16}}\\
\delta_j^l &= \sum_{k=1}^{r} \delta_k^{l+1} \cdot w_{kj}^{l+1} \cdot \sigma'({z_j^l}) && \text{by,  } {o=l+1} \tag{20} \label{eq20}\\
\end{align} %]]></script>

<p>Similarly you can easily prove for bias,
<script type="math/tex">% <![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{b_j^l}} &=\delta_j^l \tag{21} \label{eq21}\\
\frac{\partial{J}}{\partial{b_k^o}} &=\delta_k^o \tag{22} \label{eq22}\\
\end{align} %]]></script></p>

<h6 id="backword-pass-vectorized">Backword pass (Vectorized)</h6>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
\delta^o &= \nabla_{y'}J \odot \sigma'(z^o) && \text{by \eqref{eq17}} \tag{23} \label{eq23}\\
\delta^l &= ((W^{l+1})^T \otimes \delta^{l+1} )\odot  \sigma'({z^l}) && \text{by \eqref{eq20}} \tag{24} \label{eq24}\\
\end{align} %]]></script>

<p>where <script type="math/tex">(W^l)^T</script> is transpose of matrix <script type="math/tex">W^l</script>  and <script type="math/tex">\nabla_{y'}J</script>, Derivative of J with respect recpect to vector <script type="math/tex">y'</script>, i.e.</p>

<script type="math/tex; mode=display">\begin{align} 
\nabla_{y'}J =  \frac{\partial{J}}{\partial{y'}}= \left[\begin{array}{c} \frac{\partial{J}}{y'_1} \\  \frac{\partial{J}}{y'_2} \\ \vdots \\ \frac{\partial{J}}{y'_k} \\ \vdots \end{array}\right]
\end{align}</script>

<p>Also,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\nabla_{b^l}J &=  \delta^l   && \text{by \eqref{eq21}} \tag{25} \label{eq25}\\
\nabla_{W^l}J &=  \delta^l  \otimes (h^{l-1})^T && \text{by \eqref{eq19}} \tag{26} \label{eq26}\\
\end{align} %]]></script>

<p>Where, <script type="math/tex">\nabla_{W^l}J</script>, Derivative of J with respect recpect to matrix <script type="math/tex">W^l</script>, i.e.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\nabla_{W^l}J =  \frac{\partial{J}}{\partial{W^l}}  = \begin{bmatrix} \frac{\partial{J}}{\partial{w_{11}}} & \frac{\partial{J}}{\partial{w_{12}}} & \cdots & \frac{\partial{J}}{\partial{w_{1i}}} & \cdots \\ \frac{\partial{J}}{\partial{w_{21}}} & \frac{\partial{J}}{\partial{w_{22}}} & \cdots & \frac{\partial{J}}{\partial{w_{2i}}} & \cdots \\ \vdots & \vdots & \vdots & \vdots \\ \frac{\partial{J}}{\partial{w_{j1}}} & \frac{\partial{J}}{\partial{w_{j2}}} & \cdots & \frac{\partial{J}}{\partial{w_{ji}}} & \cdots \\ \vdots & \vdots & \vdots & \vdots \end{bmatrix}
\end{align} %]]></script>

<h4 id="update-step">Update step:</h4>
<ol>
  <li>Given a training batch of size <script type="math/tex">m</script> and learning rate <script type="math/tex">\eta</script></li>
  <li>For each training example in <script type="math/tex">i</script> in batch do <strong>Forward pass</strong> and Backward pass, accumulate <script type="math/tex">\nabla_{b^l}J^{(i)}</script> and <script type="math/tex">\nabla_{W^l}J^{(i)}</script></li>
  <li>Update weights and bias for each layer <script type="math/tex">l</script> as follows,</li>
</ol>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
b_l &= b_l -\eta \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} \nabla_{b^l}J^{(i)} \tag{27} \label{eq27}\\
W_l &= W_l - \eta \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} \nabla_{W^l}J^{(i)} \tag{28} \label{eq28}\\
\end{align} %]]></script>

<h4 id="tddr">td;dr.</h4>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h^l &= \sigma(z^l) && \text{by \eqref{eq7}} \\
z^l &= W^l \otimes h^{l-1} + b^l && \text{by \eqref{eq8}}  \\
\delta^o &= \nabla_{y'}J \odot \sigma'(z^o) && \text{by \eqref{eq23}} \\
\delta^l &= ((W^{l+1})^T \otimes \delta^{l+1} )\odot  \sigma'({z^l}) && \text{by \eqref{eq24}} \\
\nabla_{b^l}J &=  \delta^l   && \text{by \eqref{eq26}} \\
\nabla_{W^l}J &=  \delta^l  \otimes (h^{l-1})^T && \text{by \eqref{eq26}} \\
b_l &= b_l -\eta \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} \nabla_{b^l}J^{(i)} && \text{by \eqref{eq27}}  \\
W_l &= W_l - \eta \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} \nabla_{W^l}J^{(i)} && \text{by \eqref{eq28}} \\
\end{align} %]]></script>

<h4 id="code"><a href="https://github.com/rakesh-malviya/MLCodeGems/tree/master/Projects/Neural_networks/src">Code</a></h4>

<p><a href="https://github.com/rakesh-malviya/MLCodeGems/tree/master/Projects/Neural_networks/src">Here</a> is the python implementation of the above article.</p>

<h4 id="references">References:</h4>
<ol>
  <li>Neural Networks and Deep Learning Chapter 2 <a href="http://neuralnetworksanddeeplearning.com/chap2.html">link</a></li>
</ol>

</div>
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v2.10";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>
        <div class="fb-share-button" data-href="/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html" data-layout="button_count" data-size="small" data-mobile-iframe="true"><a class="fb-xfbml-parse-ignore" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdevelopers.facebook.com%2Fdocs%2Fplugins%2F&amp;src=sdkpreparse">Share</a></div>

        <a href="https://twitter.com/share" class="twitter-share-button" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

        <!-- Place this tag in your head or just before your close body tag. -->
        <script src="https://apis.google.com/js/platform.js" async defer></script>
        <!-- Place this tag where you want the share button to render. -->
        <div class="g-plus" data-action="share"></div>
        <hr/>


<script id="dsq-count-scr" src="//rakesh-malviya-blog.disqus.com/count.js" async></script>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://rakesh-malviya-blog.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>                            

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-106693130-1', 'auto');
  ga('send', 'pageview');

</script>


  </div>
  <div class="footer">
  |<a href="/"> ~ </a>|
 </div> 
</body>
</html>
