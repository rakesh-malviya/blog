<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>1. Word2vec Part 1: Basics</title>
  <link href="/fonts.css" rel="stylesheet" charset="utf-8">
  <link rel="stylesheet" href="/style.css">
  <!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Word2vec Part 1: Basics | Rakesh Malviya</title>
<meta property="og:title" content="Word2vec Part 1: Basics" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="For NLP with Deep learning there is need to represent text into data that can be understood by Neural networks. One Hot encoding In this encoding we represent word vectors by zeros and ones. For e.g. assume we have following text (or corpus): “I like Deep learning and NLP” Than our vocabulary is as follows: Word Word Index   0 I 1 like 2 Deep 3 learning 4 and 5 NLP Than one-hot representation of each word will be as follows: Word One-hot I 100000 like 010000 Deep 001000 learning 000100 and 000010 NLP 000001 Notice the position of word in vocabulary and position of the bit set in its one hot representation Under this representation, each word is Independent. It hard find its relationship with other words in Corpus. Because of this one hot is also called Local representation Word2Vec Intuition “You shall know a word by the company it keeps” - By J.R. Firth 1957” You can get lot of value by representing a word by means of its neighbors. This way of representing a word given the distributions of its neighbors is called Distributional similarity based representation Word2Vec is also called Distributed representation because of how it is different from “One Hot” representation which is local. government debt problems turning into banking crises as has happened in saying that Europe needs unified banking regulation to replace the hodgepodge In above two sentences words in bold describe the word “banking” Warning Note: Below example gives only a partial picture of how word2vec algorithm can understands similarity between words but this is not how this algorithm is implemented: Let us assume we have the following sentences in our text. 1. Sam is **good** boy 2. Sam is **fine** boy 3. Sam is **great** boy Given the neighbor words &quot;Sam&quot;, &quot;is&quot;, and &quot;boy&quot; algorithms understands that &quot;good&quot;, &quot;fine&quot;, and &quot;great&quot; are similar words Word2Vec algorithm can be implemented in 2 ways: Skip Gram model CBOW (Continuous Bag-of-word) Model Skip Gram model Let us assume we have following text (or corpus): “I like Deep learning and NLP” Word vec Word I like deep learning and NLP Skip gram defines a model that predicts context words given center word . So the skip gram model trains to maximize probability of context word (neighbor words) given center words. i.e. For t = 0 center word, = = (“I”) let window for neighbors is 5 so, context words = = (“like”,”deep”) For t= 2: center word, = = (“deep”) let window for neighbors is 5 so, context words = = (“I”, “like”, “learning”, “and”) So we can start by initialize random words vectors than adjust there values while training to maximize probability or minimize loss function Algorithm steps: Look at different positions of . Calculate loss function Adjust values of word vectors CBOW (Continuous Bag-of-word) Model CBOW defines a model that predicts center word given context words. So CBOW model trains to maximizeprobability of context word (neighbor words) given center words. i.e. For us humans it is very easy to predict fill in the blanks like below: The Leopard ____ very fast Next ? In next posts we will dive deep into word2vec derivations and algorithms References Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013. Stanford CS224n: Natural Language Processing with Deep Learning" />
<meta property="og:description" content="For NLP with Deep learning there is need to represent text into data that can be understood by Neural networks. One Hot encoding In this encoding we represent word vectors by zeros and ones. For e.g. assume we have following text (or corpus): “I like Deep learning and NLP” Than our vocabulary is as follows: Word Word Index   0 I 1 like 2 Deep 3 learning 4 and 5 NLP Than one-hot representation of each word will be as follows: Word One-hot I 100000 like 010000 Deep 001000 learning 000100 and 000010 NLP 000001 Notice the position of word in vocabulary and position of the bit set in its one hot representation Under this representation, each word is Independent. It hard find its relationship with other words in Corpus. Because of this one hot is also called Local representation Word2Vec Intuition “You shall know a word by the company it keeps” - By J.R. Firth 1957” You can get lot of value by representing a word by means of its neighbors. This way of representing a word given the distributions of its neighbors is called Distributional similarity based representation Word2Vec is also called Distributed representation because of how it is different from “One Hot” representation which is local. government debt problems turning into banking crises as has happened in saying that Europe needs unified banking regulation to replace the hodgepodge In above two sentences words in bold describe the word “banking” Warning Note: Below example gives only a partial picture of how word2vec algorithm can understands similarity between words but this is not how this algorithm is implemented: Let us assume we have the following sentences in our text. 1. Sam is **good** boy 2. Sam is **fine** boy 3. Sam is **great** boy Given the neighbor words &quot;Sam&quot;, &quot;is&quot;, and &quot;boy&quot; algorithms understands that &quot;good&quot;, &quot;fine&quot;, and &quot;great&quot; are similar words Word2Vec algorithm can be implemented in 2 ways: Skip Gram model CBOW (Continuous Bag-of-word) Model Skip Gram model Let us assume we have following text (or corpus): “I like Deep learning and NLP” Word vec Word I like deep learning and NLP Skip gram defines a model that predicts context words given center word . So the skip gram model trains to maximize probability of context word (neighbor words) given center words. i.e. For t = 0 center word, = = (“I”) let window for neighbors is 5 so, context words = = (“like”,”deep”) For t= 2: center word, = = (“deep”) let window for neighbors is 5 so, context words = = (“I”, “like”, “learning”, “and”) So we can start by initialize random words vectors than adjust there values while training to maximize probability or minimize loss function Algorithm steps: Look at different positions of . Calculate loss function Adjust values of word vectors CBOW (Continuous Bag-of-word) Model CBOW defines a model that predicts center word given context words. So CBOW model trains to maximizeprobability of context word (neighbor words) given center words. i.e. For us humans it is very easy to predict fill in the blanks like below: The Leopard ____ very fast Next ? In next posts we will dive deep into word2vec derivations and algorithms References Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013. Stanford CS224n: Natural Language Processing with Deep Learning" />
<link rel="canonical" href="http://localhost:4000/2017/08/15/word2vec-basics.html" />
<meta property="og:url" content="http://localhost:4000/2017/08/15/word2vec-basics.html" />
<meta property="og:site_name" content="Rakesh Malviya" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-08-15T00:00:00+05:30" />
<script type="application/ld+json">
{"name":null,"description":"For NLP with Deep learning there is need to represent text into data that can be understood by Neural networks. One Hot encoding In this encoding we represent word vectors by zeros and ones. For e.g. assume we have following text (or corpus): “I like Deep learning and NLP” Than our vocabulary is as follows: Word Word Index   0 I 1 like 2 Deep 3 learning 4 and 5 NLP Than one-hot representation of each word will be as follows: Word One-hot I 100000 like 010000 Deep 001000 learning 000100 and 000010 NLP 000001 Notice the position of word in vocabulary and position of the bit set in its one hot representation Under this representation, each word is Independent. It hard find its relationship with other words in Corpus. Because of this one hot is also called Local representation Word2Vec Intuition “You shall know a word by the company it keeps” - By J.R. Firth 1957” You can get lot of value by representing a word by means of its neighbors. This way of representing a word given the distributions of its neighbors is called Distributional similarity based representation Word2Vec is also called Distributed representation because of how it is different from “One Hot” representation which is local. government debt problems turning into banking crises as has happened in saying that Europe needs unified banking regulation to replace the hodgepodge In above two sentences words in bold describe the word “banking” Warning Note: Below example gives only a partial picture of how word2vec algorithm can understands similarity between words but this is not how this algorithm is implemented: Let us assume we have the following sentences in our text. 1. Sam is **good** boy 2. Sam is **fine** boy 3. Sam is **great** boy Given the neighbor words &quot;Sam&quot;, &quot;is&quot;, and &quot;boy&quot; algorithms understands that &quot;good&quot;, &quot;fine&quot;, and &quot;great&quot; are similar words Word2Vec algorithm can be implemented in 2 ways: Skip Gram model CBOW (Continuous Bag-of-word) Model Skip Gram model Let us assume we have following text (or corpus): “I like Deep learning and NLP” Word vec Word I like deep learning and NLP Skip gram defines a model that predicts context words given center word . So the skip gram model trains to maximize probability of context word (neighbor words) given center words. i.e. For t = 0 center word, = = (“I”) let window for neighbors is 5 so, context words = = (“like”,”deep”) For t= 2: center word, = = (“deep”) let window for neighbors is 5 so, context words = = (“I”, “like”, “learning”, “and”) So we can start by initialize random words vectors than adjust there values while training to maximize probability or minimize loss function Algorithm steps: Look at different positions of . Calculate loss function Adjust values of word vectors CBOW (Continuous Bag-of-word) Model CBOW defines a model that predicts center word given context words. So CBOW model trains to maximizeprobability of context word (neighbor words) given center words. i.e. For us humans it is very easy to predict fill in the blanks like below: The Leopard ____ very fast Next ? In next posts we will dive deep into word2vec derivations and algorithms References Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013. Stanford CS224n: Natural Language Processing with Deep Learning","author":null,"@type":"BlogPosting","url":"http://localhost:4000/2017/08/15/word2vec-basics.html","image":null,"publisher":null,"headline":"Word2vec Part 1: Basics","dateModified":"2017-08-15T00:00:00+05:30","datePublished":"2017-08-15T00:00:00+05:30","sameAs":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017/08/15/word2vec-basics.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>
<body>
  <div class="container_post">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<header class="masthead">
  <h1 class="masthead-title--small">
    <a href="/">Rakesh Malviya</a>
  </h1>
</header>      
<div class="content post">
  <h1 class="post-title">1. Word2vec Part 1: Basics</h1>
  <div class="meta_wrapper">
  <span class="post-date">15 Aug 2017</span>
  
  
    <a href="/tags#nlp" class="post-tag">NLP</a>
  
  
<div class="meta_wrapper">  
  <p>For NLP with Deep learning there is need to represent text into data that can be understood by Neural networks.</p>

<h2 id="one-hot-encoding">One Hot encoding</h2>

<p>In this encoding we represent word vectors by zeros and ones.</p>

<p>For e.g.  assume we have following text (or corpus): <strong>“I like Deep learning and NLP”</strong></p>

<p>Than our vocabulary is as follows:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Word</th>
      <th style="text-align: left">Word</th>
    </tr>
    <tr>
      <th style="text-align: left">Index</th>
      <th style="text-align: left"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">0</td>
      <td style="text-align: left">I</td>
    </tr>
    <tr>
      <td style="text-align: left">1</td>
      <td style="text-align: left">like</td>
    </tr>
    <tr>
      <td style="text-align: left">2</td>
      <td style="text-align: left">Deep</td>
    </tr>
    <tr>
      <td style="text-align: left">3</td>
      <td style="text-align: left">learning</td>
    </tr>
    <tr>
      <td style="text-align: left">4</td>
      <td style="text-align: left">and</td>
    </tr>
    <tr>
      <td style="text-align: left">5</td>
      <td style="text-align: left">NLP</td>
    </tr>
  </tbody>
</table>

<p>Than <strong>one-hot representation</strong> of each word will be as follows:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Word</th>
      <th style="text-align: left">One-hot</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">I</td>
      <td style="text-align: left">100000</td>
    </tr>
    <tr>
      <td style="text-align: left">like</td>
      <td style="text-align: left">010000</td>
    </tr>
    <tr>
      <td style="text-align: left">Deep</td>
      <td style="text-align: left">001000</td>
    </tr>
    <tr>
      <td style="text-align: left">learning</td>
      <td style="text-align: left">000100</td>
    </tr>
    <tr>
      <td style="text-align: left">and</td>
      <td style="text-align: left">000010</td>
    </tr>
    <tr>
      <td style="text-align: left">NLP</td>
      <td style="text-align: left">000001</td>
    </tr>
  </tbody>
</table>

<p>Notice the position of word in vocabulary and position of the bit set in its one hot representation</p>

<p>Under this representation, each word is Independent. It hard find its relationship with other words in Corpus. Because of this one hot is also called <strong>Local representation</strong></p>

<h2 id="word2vec-intuition">Word2Vec Intuition</h2>

<p><strong>“You shall know a word by the company it keeps” - By J.R. Firth 1957”</strong></p>

<p>You can get lot of value by representing a word by means of its neighbors. This way of representing a word given the distributions of its neighbors is called <strong>Distributional similarity based representation</strong></p>

<p>Word2Vec is also called <strong>Distributed representation</strong> because of how it is different from “One Hot” representation which is local.</p>

<ol>
  <li><strong>government debt problems turning into</strong> banking <strong>crises as has happened in</strong></li>
  <li><strong>saying that Europe needs unified</strong> banking <strong>regulation to replace the hodgepodge</strong><br />
In above two sentences words in <strong>bold</strong> describe the word “banking”</li>
</ol>

<p><strong>Warning Note:</strong> Below example gives only a partial picture of how word2vec algorithm can understands similarity between words but this is not how this algorithm is implemented:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Let us assume we have the following sentences in our text.

1. Sam is **good** boy
2. Sam is **fine** boy
3. Sam is **great** boy

Given the neighbor words "Sam", "is", and "boy" algorithms understands that "good", "fine", and "great" are similar words
</code></pre>
</div>

<p>Word2Vec algorithm can be implemented in 2 ways:</p>

<ol>
  <li>Skip Gram model</li>
  <li>CBOW (Continuous Bag-of-word) Model</li>
</ol>

<h2 id="skip-gram-model">Skip Gram model</h2>

<p>Let us assume we have following text (or corpus): <strong>“I like Deep learning and NLP”</strong></p>

<table>
  <thead>
    <tr>
      <th>Word vec</th>
      <th>Word</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">w_0</script></td>
      <td><strong>I</strong></td>
    </tr>
    <tr>
      <td><script type="math/tex">w_1</script></td>
      <td><strong>like</strong></td>
    </tr>
    <tr>
      <td><script type="math/tex">w_2</script></td>
      <td><strong>deep</strong></td>
    </tr>
    <tr>
      <td><script type="math/tex">w_3</script></td>
      <td><strong>learning</strong></td>
    </tr>
    <tr>
      <td><script type="math/tex">w_4</script></td>
      <td><strong>and</strong></td>
    </tr>
    <tr>
      <td><script type="math/tex">w_5</script></td>
      <td><strong>NLP</strong></td>
    </tr>
  </tbody>
</table>

<p>Skip gram defines a model that predicts context words given center word <script type="math/tex">w_t</script>. So the skip gram model trains to maximize probability of context word (neighbor words) given center words. <br />
i.e. <script type="math/tex">P(w_{c}  \vert  w_t)</script></p>

<p>For  t = 0<br />
center word, <script type="math/tex">w_t</script> =<script type="math/tex">w_0</script> = (“I”)<br />
let window for neighbors is 5 so,<br />
context words = <script type="math/tex">w_{c}</script> = (“like”,”deep”)</p>

<p>For t= 2:<br />
center word, <script type="math/tex">w_t</script> = <script type="math/tex">w_2</script> = (“deep”)<br />
let window for neighbors is 5 so,<br />
context words = <script type="math/tex">w_{c}</script> = (“I”, “like”, “learning”, “and”)</p>

<p>So we can start by initialize random words vectors than adjust there values while training to maximize probability  <script type="math/tex">P(w_c \vert w_t)</script> or minimize loss function <script type="math/tex">J =1- P(w_c \vert w_t)</script></p>

<h4 id="algorithm-steps">Algorithm steps:</h4>
<ol>
  <li>Look at different positions of <script type="math/tex">t</script>.</li>
  <li>Calculate loss function <script type="math/tex">J =1- P(w_c \vert w_t)</script></li>
  <li>Adjust values of word vectors <script type="math/tex">w_t</script></li>
</ol>

<h2 id="cbow-continuous-bag-of-word-model">CBOW (Continuous Bag-of-word) Model</h2>
<p>CBOW defines a model that predicts center word <script type="math/tex">w_t</script> given context words. So CBOW model trains to maximizeprobability of context word (neighbor words) given center words. <br />
i.e. <script type="math/tex">P(w_t \vert w_c)</script></p>

<div class="highlighter-rouge"><pre class="highlight"><code>For us humans it is very easy to predict fill in the blanks like below:
The Leopard ____ very fast
</code></pre>
</div>

<h2 id="next-">Next ?</h2>
<p>In next posts we will dive deep into word2vec derivations and algorithms</p>

<h2 id="references">References</h2>

<ol>
  <li>
    <p>Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013.</p>
  </li>
  <li>
    <p>Stanford CS224n: Natural Language Processing with Deep Learning</p>
  </li>
</ol>

</div>

<script id="dsq-count-scr" src="//rakesh-malviya-blog.disqus.com/count.js" async></script>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://rakesh-malviya-blog.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>                            

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-106693130-1', 'auto');
  ga('send', 'pageview');

</script>


  </div>
  <div class="footer">
  |<a href="/"> ~ </a>|
 </div> 
</body>
</html>
