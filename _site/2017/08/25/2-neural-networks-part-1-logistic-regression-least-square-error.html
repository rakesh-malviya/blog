<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>2. Neural Networks Part 1: Logistic Regression (Least Square Error)</title>
  <link href="/fonts.css" rel="stylesheet" charset="utf-8">
  <link rel="stylesheet" href="/style.css">
  <!-- Begin Jekyll SEO tag v2.2.3 -->
<title>Neural Networks Part 1: Logistic Regression (Least Square Error) | Rakesh Malviya</title>
<meta property="og:title" content="Neural Networks Part 1: Logistic Regression (Least Square Error)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Required Learning: Linear regression basics link We are starting from basic unit of Neural networks single activation neuron. A Neural network with single neuron is same as logistic regression. Therefore a neural network can be considered as a networked set of logistic regression units. Establish notations for future use1 to denote the ith “input ” of training data to denote the ith “output” or target of training data Pair is called a training example The dataset that we’ll be using to learn—a list of m training examples — is called a training set Each in training set can have features such that is a vector In current setup of logistic regression is scalar value Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. Fig: Single neuron Let , where is predicted output is activation function , for training example, where is bias of the neuron. weights or training parameters we need to learn We will apply gradient descent to minimize the squared error cost function , also called least square error \begin{align} J = \sum_{i=1}^{m} {\dfrac{1}{2}}(y^{(i)} - y’^{(i)})^2 \tag{1} \label{eq1} \end{align} We will use Sigmoid function as activation function, i.e. \begin{align} \sigma(u^{(i)}) = \dfrac{1}{1+e^{-u^{(i)}}} = f(u^{(i)}) = y’^{(i)} \tag{2} \label{eq2} \end{align} Derivatives \begin{align} \frac{\partial\sigma(u)}{\partial{u}} = \sigma(u)\cdot(1 - \sigma(u)) \tag{3} \label{eq3} \end{align} \begin{align} \frac{\partial{J}}{\partial{y’^{(i)}}} = y’^{(i)} - y^{(i)} \tag{4} \label{eq4} \end{align} \begin{align} \frac{\partial{u^{(i)}}}{\partial{w_j}} = x_j^{(i)} \tag{5} \label{eq5} \end{align} \begin{align} \frac{\partial{u^{(i)}}}{\partial{b}} = 1 \tag{6} \label{eq6} \end{align} Gradient Descent To learn and parameter so that is minimum. We will find and Note: is our loss function and is for indexing Since, is function of , is function of and is function of by . Using chain rule of differentiation So during the training update equation for over all is as follows: Similarly: Update equation for bias is as follows: Training Steps: Choose an initial vector of parameters , bias and learning rate . Repeat for predifined epoch such that approximate minimum loss is obtained: Evaluate and store for all training examples by using equation Update bias, For in : Update, here is the python implementation of this article. References: http://cs229.stanford.edu/notes Previous Posts: “1. Word2vec Part 1: Basics”" />
<meta property="og:description" content="Required Learning: Linear regression basics link We are starting from basic unit of Neural networks single activation neuron. A Neural network with single neuron is same as logistic regression. Therefore a neural network can be considered as a networked set of logistic regression units. Establish notations for future use1 to denote the ith “input ” of training data to denote the ith “output” or target of training data Pair is called a training example The dataset that we’ll be using to learn—a list of m training examples — is called a training set Each in training set can have features such that is a vector In current setup of logistic regression is scalar value Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. Fig: Single neuron Let , where is predicted output is activation function , for training example, where is bias of the neuron. weights or training parameters we need to learn We will apply gradient descent to minimize the squared error cost function , also called least square error \begin{align} J = \sum_{i=1}^{m} {\dfrac{1}{2}}(y^{(i)} - y’^{(i)})^2 \tag{1} \label{eq1} \end{align} We will use Sigmoid function as activation function, i.e. \begin{align} \sigma(u^{(i)}) = \dfrac{1}{1+e^{-u^{(i)}}} = f(u^{(i)}) = y’^{(i)} \tag{2} \label{eq2} \end{align} Derivatives \begin{align} \frac{\partial\sigma(u)}{\partial{u}} = \sigma(u)\cdot(1 - \sigma(u)) \tag{3} \label{eq3} \end{align} \begin{align} \frac{\partial{J}}{\partial{y’^{(i)}}} = y’^{(i)} - y^{(i)} \tag{4} \label{eq4} \end{align} \begin{align} \frac{\partial{u^{(i)}}}{\partial{w_j}} = x_j^{(i)} \tag{5} \label{eq5} \end{align} \begin{align} \frac{\partial{u^{(i)}}}{\partial{b}} = 1 \tag{6} \label{eq6} \end{align} Gradient Descent To learn and parameter so that is minimum. We will find and Note: is our loss function and is for indexing Since, is function of , is function of and is function of by . Using chain rule of differentiation So during the training update equation for over all is as follows: Similarly: Update equation for bias is as follows: Training Steps: Choose an initial vector of parameters , bias and learning rate . Repeat for predifined epoch such that approximate minimum loss is obtained: Evaluate and store for all training examples by using equation Update bias, For in : Update, here is the python implementation of this article. References: http://cs229.stanford.edu/notes Previous Posts: “1. Word2vec Part 1: Basics”" />
<link rel="canonical" href="http://localhost:4000/2017/08/25/2-neural-networks-part-1-logistic-regression-least-square-error.html" />
<meta property="og:url" content="http://localhost:4000/2017/08/25/2-neural-networks-part-1-logistic-regression-least-square-error.html" />
<meta property="og:site_name" content="Rakesh Malviya" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-08-25T00:00:00+05:30" />
<script type="application/ld+json">
{"@context":"http://schema.org","@type":"BlogPosting","headline":"Neural Networks Part 1: Logistic Regression (Least Square Error)","datePublished":"2017-08-25T00:00:00+05:30","dateModified":"2017-08-25T00:00:00+05:30","description":"Required Learning: Linear regression basics link We are starting from basic unit of Neural networks single activation neuron. A Neural network with single neuron is same as logistic regression. Therefore a neural network can be considered as a networked set of logistic regression units. Establish notations for future use1 to denote the ith “input ” of training data to denote the ith “output” or target of training data Pair is called a training example The dataset that we’ll be using to learn—a list of m training examples — is called a training set Each in training set can have features such that is a vector In current setup of logistic regression is scalar value Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. Fig: Single neuron Let , where is predicted output is activation function , for training example, where is bias of the neuron. weights or training parameters we need to learn We will apply gradient descent to minimize the squared error cost function , also called least square error \\begin{align} J = \\sum_{i=1}^{m} {\\dfrac{1}{2}}(y^{(i)} - y’^{(i)})^2 \\tag{1} \\label{eq1} \\end{align} We will use Sigmoid function as activation function, i.e. \\begin{align} \\sigma(u^{(i)}) = \\dfrac{1}{1+e^{-u^{(i)}}} = f(u^{(i)}) = y’^{(i)} \\tag{2} \\label{eq2} \\end{align} Derivatives \\begin{align} \\frac{\\partial\\sigma(u)}{\\partial{u}} = \\sigma(u)\\cdot(1 - \\sigma(u)) \\tag{3} \\label{eq3} \\end{align} \\begin{align} \\frac{\\partial{J}}{\\partial{y’^{(i)}}} = y’^{(i)} - y^{(i)} \\tag{4} \\label{eq4} \\end{align} \\begin{align} \\frac{\\partial{u^{(i)}}}{\\partial{w_j}} = x_j^{(i)} \\tag{5} \\label{eq5} \\end{align} \\begin{align} \\frac{\\partial{u^{(i)}}}{\\partial{b}} = 1 \\tag{6} \\label{eq6} \\end{align} Gradient Descent To learn and parameter so that is minimum. We will find and Note: is our loss function and is for indexing Since, is function of , is function of and is function of by . Using chain rule of differentiation So during the training update equation for over all is as follows: Similarly: Update equation for bias is as follows: Training Steps: Choose an initial vector of parameters , bias and learning rate . Repeat for predifined epoch such that approximate minimum loss is obtained: Evaluate and store for all training examples by using equation Update bias, For in : Update, here is the python implementation of this article. References: http://cs229.stanford.edu/notes Previous Posts: “1. Word2vec Part 1: Basics”","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017/08/25/2-neural-networks-part-1-logistic-regression-least-square-error.html"},"url":"http://localhost:4000/2017/08/25/2-neural-networks-part-1-logistic-regression-least-square-error.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
<body>
  <div class="container_post">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<header class="masthead">
  <h1 class="masthead-title--small">
    <a href="/">Rakesh Malviya</a>
  </h1>
</header>      
<div class="content post">
  <h1 class="post-title">2. Neural Networks Part 1: Logistic Regression (Least Square Error)</h1>
  <div class="meta_wrapper">
  <div class="post-date">
    <time>25 Aug 2017</time>
  </div>
  <div>
    
    <u><a class="tag_list_link" href="/tag/Neural Networks">Neural Networks</a></u>
    &nbsp;&nbsp;
    
    <u><a class="tag_list_link" href="/tag/Machine Learning">Machine Learning</a></u>
    &nbsp;&nbsp;
    
    <u><a class="tag_list_link" href="/tag/Deep Learning">Deep Learning</a></u>
    &nbsp;&nbsp;
    
  </div>
  <hr>
<div class="meta_wrapper">  
  <p>Required Learning: Linear regression basics <a href="http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html">link</a></p>

<p>We are starting from basic unit of Neural networks single activation neuron. A Neural network with single neuron is same as logistic regression. Therefore a neural network can be considered as a networked set of logistic regression units.</p>

<h3 id="establish-notations-for-future-use1">Establish notations for future use<sup><a href="#references">1</a></sup></h3>
<ol>
  <li><script type="math/tex">x^{(i)}</script> to denote the i<sup>th</sup> “input ” of training data</li>
  <li><script type="math/tex">y^{(i)}</script> to denote the i<sup>th</sup> “output” or target of training data</li>
  <li>Pair <script type="math/tex">(x^{(i)}, y^{(i)})</script> is called a training example</li>
  <li>The dataset that we’ll be using to learn—a list of m training examples <script type="math/tex">\{(x(i), y(i)); i = 1, . . . , m\}</script> — is called a training set</li>
  <li>Each <script type="math/tex">x^{(i)}</script> in training set can have <script type="math/tex">n</script> <strong>features</strong> such that <script type="math/tex">x^{(i)}</script> is a vector <script type="math/tex">(x^{(i)}_1,x^{(i)}_2,x^{(i)}_3,..... x^{(i)}_n)</script></li>
  <li>In current setup of logistic regression <script type="math/tex">y^{(i)}</script> is scalar value</li>
</ol>

<p><strong>Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.</strong></p>

<p><img src="/assets/img/blog/2/2_1_logistic_reg.svg" alt="" title="Single neuron" />
<i><center>Fig: Single neuron</center></i></p>

<p>Let <script type="math/tex">y'^{(i)} = f(u^{(i)})</script> , 
where</p>
<ol>
  <li><script type="math/tex">y'^{(i)}</script> is predicted output</li>
  <li><script type="math/tex">f</script> is activation function</li>
  <li><script type="math/tex">u^{(i)} = {b} + \sum_{j=1}^{n} {w_j}\cdot{x_j^{(i)}}</script>, for <script type="math/tex">i^{th}</script> training example, where <script type="math/tex">b</script> is bias of the neuron.</li>
  <li><script type="math/tex">w_j</script> <strong>weights</strong> or <strong>training parameters</strong> we need to learn</li>
</ol>

<p>We will apply gradient descent to minimize the squared error cost function <script type="math/tex">J</script>, also called least square error</p>

<p>\begin{align}
J = \sum_{i=1}^{m} {\dfrac{1}{2}}(y^{(i)} - y’^{(i)})^2 \tag{1} \label{eq1}
\end{align}</p>

<p>We will use Sigmoid function as activation function, i.e. <script type="math/tex">\sigma(u)</script>
\begin{align}
\sigma(u^{(i)}) = \dfrac{1}{1+e^{-u^{(i)}}} = f(u^{(i)}) = y’^{(i)}   \tag{2} \label{eq2}
\end{align}</p>

<h4 id="derivatives">Derivatives</h4>

<p>\begin{align}
\frac{\partial\sigma(u)}{\partial{u}} = \sigma(u)\cdot(1 - \sigma(u))   \tag{3} \label{eq3}
\end{align}
\begin{align}
\frac{\partial{J}}{\partial{y’^{(i)}}} = y’^{(i)} - y^{(i)}   \tag{4} \label{eq4}
\end{align}
\begin{align}
\frac{\partial{u^{(i)}}}{\partial{w_j}} = x_j^{(i)}   \tag{5} \label{eq5}
\end{align}
\begin{align}
\frac{\partial{u^{(i)}}}{\partial{b}} = 1   \tag{6} \label{eq6}
\end{align}</p>

<h4 id="gradient-descent">Gradient Descent</h4>

<p>To learn <script type="math/tex">w_j</script> and <script type="math/tex">b</script> parameter so that <script type="math/tex">J</script> is minimum. We will find <script type="math/tex">\frac{\partial{J}}{\partial{w_j}}</script> and <script type="math/tex">\frac{\partial{J}}{\partial{b}}</script></p>

<p><strong>Note: <script type="math/tex">J</script> is our loss function and <script type="math/tex">j</script> is for indexing</strong></p>

<p>Since, <script type="math/tex">J</script> is function of <script type="math/tex">y'^{(i)}</script>, <script type="math/tex">y'^{(i)}</script> is function of <script type="math/tex">u^{(i)}</script> and <script type="math/tex">u^{(i)}</script> is function of <script type="math/tex">w_j</script> by <script type="math/tex">\eqref{eq1} and \eqref{eq2}</script>. Using chain rule of differentiation</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{w_j}} &= \sum_{i=1}^{m}  \frac{\partial{J}}{\partial{y'^{(i)}}} \cdot  \frac{\partial{y'^{(i)}}}{\partial{w_j}}   && \text{by \eqref{eq1}} \\
&= \sum_{i=1}^{m}  \frac{\partial{J}}{\partial{y'^{(i)}}} \cdot  \frac{\partial{y'^{(i)}}}{\partial{u^{(i)}}} \cdot    \frac{\partial{u^{(i)}}}{\partial{w_j}}  \\
&= \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  \frac{\partial{y'^{(i)}}}{\partial{u^{(i)}}} \cdot    \frac{\partial{u^{(i)}}}{\partial{w_j}}  && \text{by \eqref{eq4}} \\
&= \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot    \frac{\partial{u^{(i)}}}{\partial{w_j}}  && \text{by \eqref{eq2} and \eqref{eq3}} \\
\frac{\partial{J}}{\partial{w_j}} &= \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot  x_j^{(i)} && \text{by \eqref{eq5}} \tag{7} \label{eq7}
\end{align} %]]></script>

<p>So during the training update equation for <script type="math/tex">w_j</script> over all <script type="math/tex">j = 1 ..... n</script> is as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
w_j &= w_j - \eta \cdot \frac{\partial{J}}{\partial{w_j}} \\
&= w_j - \eta \cdot \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot  x_j^{(i)} && \text{by \eqref{eq7}}
\end{align} %]]></script>

<p>Similarly:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{b}} &= \sum_{i=1}^{m}  \frac{\partial{J}}{\partial{y'^{(i)}}} \cdot  \frac{\partial{y'^{(i)}}}{\partial{b}}   && \text{by \eqref{eq1}} \\
&= \sum_{i=1}^{m}  \frac{\partial{J}}{\partial{y'^{(i)}}} \cdot  \frac{\partial{y'^{(i)}}}{\partial{u^{(i)}}} \cdot    \frac{\partial{u^{(i)}}}{\partial{b}}  \\
&= \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  \frac{\partial{y'^{(i)}}}{\partial{u^{(i)}}} \cdot    \frac{\partial{u^{(i)}}}{\partial{b}}  && \text{by \eqref{eq4}} \\
&= \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot    \frac{\partial{u^{(i)}}}{\partial{b}}  && \text{by \eqref{eq2} and \eqref{eq3}} \\
\frac{\partial{J}}{\partial{b}} &= \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) && \text{by \eqref{eq6}} \tag{8} \label{eq8}
\end{align} %]]></script>

<p>Update equation for bias <script type="math/tex">b</script> is as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
b &= b - \eta \cdot \frac{\partial{J}}{\partial{w_j}} \\
&= b - \eta \cdot \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) && \text{by \eqref{eq8}}
\end{align} %]]></script>

<p>Training Steps:</p>
<ol>
  <li>Choose an initial vector of parameters  <script type="math/tex">w = (w_1,......w_n)</script>, bias <script type="math/tex">b</script> and learning rate <script type="math/tex">\eta</script>.</li>
  <li>Repeat for predifined epoch such that approximate minimum <script type="math/tex">J</script> loss is obtained:
    <ol>
      <li>Evaluate and store <script type="math/tex">y'^{(i)}</script> for all <script type="math/tex">i = 1,2,3...m</script> training examples by using equation <script type="math/tex">\eqref{eq2}</script></li>
      <li>Update bias, <script type="math/tex">b = b - \eta \cdot \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)})</script></li>
      <li>For <script type="math/tex">j = 1,2,.....n</script> in <script type="math/tex">w</script> :
        <ol>
          <li>Update, <script type="math/tex">w_j = w_j - \eta \cdot \sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot  x_j^{(i)}</script></li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p><a href="https://github.com/rakesh-malviya/blog/blob/master/code/notebooks/3-neural-networks-part-1-logistic-regression-least-square-error.ipynb">here</a> is the python implementation of this article.</p>

<h2 id="references">References:</h2>
<ol>
  <li>http://cs229.stanford.edu/notes</li>
</ol>

<h2 id="previous-posts">Previous Posts:</h2>
<p><a href="/2017/08/15/word2vec-basics.html">“1. Word2vec Part 1: Basics”</a></p>

</div>

  </div>
  <div class="footer">
  |<a href="/"> ~ </a>|
 </div> 
</body>
</html>
