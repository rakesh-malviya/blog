<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
	<channel>
		<title>Rakesh Malviya</title>
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>4. Neural Networks Part 3: Feedforward neural network</title>
        <description>&lt;p&gt;&lt;strong&gt;Required Learning:&lt;/strong&gt; My previous posts on Neural networks&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Feedforward neural network&lt;/strong&gt; is neural network architecture consisting of layers of artificial neurons such that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Each neuron in a layer is connected to all neurons of previous layer, i.e. its input is output of all neurons of previous layers.&lt;/li&gt;
  &lt;li&gt;Last layer is called output layer and returns &lt;script type=&quot;math/tex&quot;&gt;y'&lt;/script&gt;, predicted y&lt;/li&gt;
  &lt;li&gt;Other layers are called hidden layers&lt;/li&gt;
  &lt;li&gt;first hidden layer‚Äôs input is &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is a vector &lt;script type=&quot;math/tex&quot;&gt;(x_1,x_2,x_3,..... x_n)&lt;/script&gt; of size &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;. &lt;strong&gt;Note:&lt;/strong&gt; Some people consider an imaginary input layer of size &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; which connects to first hidden layer. If you haven‚Äôt read this anywhere just forget this note. For others I find this way of representing layers easier when will be implementing classes for different types of neural network layers.&lt;/li&gt;
  &lt;li&gt;We can have zero to any number of hidden layers. In case of zero hidden layers our network for binary classification task will be same as logistic regression descrsibed in previous post. Think ‚Ä¶ üí≠ Since we have only single artificial neuron of output layer in the whole neural network.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/4/4_1_neuralnetwork.svg&quot; alt=&quot;&quot; title=&quot;Feedforward neural network&quot; /&gt;
&lt;i&gt;&lt;center&gt;Fig 1 : Feedforward neural network&lt;/center&gt;&lt;/i&gt;&lt;/p&gt;

&lt;h4 id=&quot;establish-notations&quot;&gt;Establish Notations:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;For notational convenience we have removed superscript (i) for &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; training example &lt;script type=&quot;math/tex&quot;&gt;(x,y)&lt;/script&gt;. So &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is single training input, &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is expected output and &lt;script type=&quot;math/tex&quot;&gt;y'&lt;/script&gt; is predicted output&lt;/li&gt;
  &lt;li&gt;Input &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is such that &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is a vector &lt;script type=&quot;math/tex&quot;&gt;(x_1,x_2,x_3,..... x_n)&lt;/script&gt; of size &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;We have &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; hidden layers. Output layer is denoted by &lt;script type=&quot;math/tex&quot;&gt;l+1&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;o&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Each layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; has following attributes weight matrix &lt;script type=&quot;math/tex&quot;&gt;W^l&lt;/script&gt;, output vector &lt;script type=&quot;math/tex&quot;&gt;h^l&lt;/script&gt; and bias vector $$b^l$.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;I am proposing below notations for your easy of understanding the equations in upcoming sections. Also check Fig 1 above&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;let output layer has size &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; i.e &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; artificial neurons. Layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; has size &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; and Layer &lt;script type=&quot;math/tex&quot;&gt;l-1&lt;/script&gt; has size &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;We will use &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; to denote individual neurons in layer &lt;script type=&quot;math/tex&quot;&gt;l-1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; for layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; for output layer &lt;script type=&quot;math/tex&quot;&gt;o&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;You need to think carefully why I am proposing below sizes for weight matrix, output vector and bias. It will help if you assume this, read understand whole article, comeback to this and think&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;For layer  &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; weight matrix &lt;script type=&quot;math/tex&quot;&gt;W^l&lt;/script&gt; has size &lt;script type=&quot;math/tex&quot;&gt;[qxp]&lt;/script&gt; and for ouput layer &lt;script type=&quot;math/tex&quot;&gt;W^o&lt;/script&gt; has size &lt;script type=&quot;math/tex&quot;&gt;[rxq]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For layer  &lt;script type=&quot;math/tex&quot;&gt;l-1&lt;/script&gt; vectors &lt;script type=&quot;math/tex&quot;&gt;h^{l-1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b^{l-1}&lt;/script&gt; has size &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and  for layer  &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; vectors &lt;script type=&quot;math/tex&quot;&gt;h^l&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b^l&lt;/script&gt; has size &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; and for output layer &lt;script type=&quot;math/tex&quot;&gt;o&lt;/script&gt; vectors &lt;script type=&quot;math/tex&quot;&gt;h^o&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b^o&lt;/script&gt; has size &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; Artificial neuron in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;  has bias &lt;script type=&quot;math/tex&quot;&gt;b_j^l&lt;/script&gt; (scalar), output &lt;script type=&quot;math/tex&quot;&gt;h_j^l&lt;/script&gt;(scalar),  weight vector &lt;script type=&quot;math/tex&quot;&gt;W_{j:}^l&lt;/script&gt; , i.e. &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; row of weight matrix &lt;script type=&quot;math/tex&quot;&gt;W^l&lt;/script&gt; for layer l. Also weight vector &lt;script type=&quot;math/tex&quot;&gt;W_{j:}^l = (w_{j1},w_{j2},....w_{ji}...,w_{jp},)&lt;/script&gt;. &lt;em&gt;It will help it you remember from my previous post on logistic reggression that each neuron has weight vector, (scalar or single value) bias and output&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Below notations are neccessary to easy of understand and implementing mathematical equations in upcoming sections&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;We will use &lt;script type=&quot;math/tex&quot;&gt;\odot&lt;/script&gt; for elementwise multiplication of two vectors, for example &lt;script type=&quot;math/tex&quot;&gt;\left[\begin{array}{c} 1 \\ 2 \end{array}\right]   \odot \left[\begin{array}{c} 1 \\ 2\end{array} \right]= \left[ \begin{array}{c} {1 \cdot 1} \\ {2 \cdot 2} \end{array} \right]= \left[ \begin{array}{c} 1 \\ 4 \end{array} \right]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;We will use &lt;script type=&quot;math/tex&quot;&gt;\otimes&lt;/script&gt; for matrix multiplication&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;training&quot;&gt;Training&lt;/h4&gt;
&lt;p&gt;Let us use our Neural network (in Fig 1) for classification. Note, for binary classification our last layer will have only one neuron hence &lt;script type=&quot;math/tex&quot;&gt;r==1&lt;/script&gt;, but we will keep our approach general. Also for simplicity we will assume that activation function of all the neurons is sigmoid function &lt;script type=&quot;math/tex&quot;&gt;\sigma()&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let our loss function, &lt;script type=&quot;math/tex&quot;&gt;J = \frac{1}{2}\sum_{k=1}^{r}(y_k - y'_k)^2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;So our goal is to minimize loss, &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt;. We can do this only by learning correct weights and bias for each neuron in our network. It looks complex if we look at each neuron individualy. We can simplify this task by breaking our training steps into three steps of &lt;strong&gt;Backpropagation&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Forward pass:&lt;/strong&gt; calculate output of each neuron.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Backward pass:&lt;/strong&gt; calculate &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial{J}}{\partial{b_j^l}}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial{J}}{\partial{w_{ji}^l}}&lt;/script&gt; for each neuron.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Update weights and biases&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;foward-pass&quot;&gt;Foward pass:&lt;/h4&gt;
&lt;p&gt;Output of &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; neuron in &lt;script type=&quot;math/tex&quot;&gt;l^{th}&lt;/script&gt; layer is defined as, 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align} 
h_j^l = \sigma(\sum_{i=1}^{p} w_{ji}^l  h_i^{l-1} + b_j^l) \tag{1} \label{eq1}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We can use equation &lt;script type=&quot;math/tex&quot;&gt;\eqref{eq1}&lt;/script&gt; to get output of each neuron in forward pass&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let total input to above neuron defined as, 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align} 
z_j^l = \sum_{i=1}^{p} w_{ji}^l  h_i^{l-1} + b_j^l \tag{2} \label{eq2}
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;hence, 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
h_j^l &amp;= \sigma(z_j^l) &amp;&amp; \text{by \eqref{eq1} and \eqref{eq2}} \tag{3} \label{eq3}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;similarly for output layer, 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
h_k^o &amp;= \sigma(z_k^o) &amp;&amp; \text{by \eqref{eq3}} \tag{4} \label{eq4}\\
z_k^o &amp;= \sum_{j=1}^{q} w_{kj}^o  h_j^{l} + b_k^o &amp;&amp; \text{by \eqref{eq2}} \tag{5} \label{eq5}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Also, output of output layer is predicted &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; hence,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align} 
h_k^o = y'_k \tag{6} \label{eq6}
\end{align}&lt;/script&gt;

&lt;h6 id=&quot;forward-pass-vectorized&quot;&gt;Forward pass (vectorized)&lt;/h6&gt;
&lt;p&gt;It is important that we implement code in the vectors and matrix operations to improve preformance.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
h^l &amp;= \sigma(z^l) &amp;&amp; \text{by \eqref{eq3}} \tag{7} \label{eq7}\\
z^l &amp;= W^l \otimes h^{l-1} + b^l &amp;&amp; \text{by \eqref{eq2}}  \tag{8} \label{eq8}\\
h^o &amp;= \sigma(z^o) &amp;&amp; \text{by \eqref{eq4}} \tag{9} \label{eq9}\\
z^o &amp;= W^o \otimes h^{l} + b^o &amp;&amp; \text{by \eqref{eq5}}  \tag{10} \label{eq10}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Note: for vectors &lt;script type=&quot;math/tex&quot;&gt;\sigma()&lt;/script&gt; is called elementwise&lt;/strong&gt;&lt;/p&gt;

&lt;h6 id=&quot;useful-derivatives&quot;&gt;Useful derivatives:&lt;/h6&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{\partial{h_j^l}}{\partial z_j^l} &amp;= \frac{\partial}{\partial z_j^l}{\sigma({z_j^l}}) &amp;&amp; \text{by \eqref{eq3}} \\
\frac{\partial{h_j^l}}{\partial z_j^l} &amp;= \sigma'({z_j^l}) &amp;&amp; \tag{11} \label{eq11} \\
\frac{\partial{z_j^l}}{\partial w_{ji}^l} &amp;= h_i^{l-1} &amp;&amp; \tag{12} \label{eq12}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where, &lt;script type=&quot;math/tex&quot;&gt;\sigma'(z)&lt;/script&gt; is derivative of &lt;script type=&quot;math/tex&quot;&gt;\sigma(z)&lt;/script&gt; with repect &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Similarly for loss &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt;,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial y'_k} &amp;= (y'_k - y_k) &amp;&amp; \text{how ? check prev post} \tag{13} \label{eq13}
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;backward-pass&quot;&gt;Backward pass&lt;/h4&gt;

&lt;p&gt;Gradient of loss with respect to weights for output layer is,&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{w_{kj}^o}} &amp;= \frac{\partial{J}}{\partial{y'_k}} \cdot \frac{\partial{y'_k}}{\partial{z_k^o}} \cdot \frac{\partial{z_k^o}}{\partial{w_{kj}^o}}  &amp;&amp;\text{by chain rule} \tag{14} \label{eq14} \\
\frac{\partial{J}}{\partial{w_{kj}^o}} &amp;=(y'_k - y_k)  \cdot  \sigma'({z_k^o}) \cdot h_j^l  &amp;&amp;\text{by \eqref{eq11},\eqref{eq12},\eqref{eq13}}  \tag{15} \label{eq15}\\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Let us define gradient of loss with respect to total input for output layer as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
\delta_k^o &amp;= \frac{\partial{J}}{\partial{z_k^o}} \tag{16} \label{eq16}\\
&amp;= \frac{\partial{J}}{\partial{y'_k}} \cdot \frac{\partial{y'_k}}{\partial{z_k^o}} \\
\delta_k^o &amp;= \frac{\partial{J}}{\partial{y'_k}} \cdot \sigma'({z_k^o}) \tag{17} \label{eq17}\\
\frac{\partial{J}}{\partial{w_{kj}^o}} &amp;=\delta_k^o \cdot h_j^l  &amp;&amp;\text{by \eqref{eq15},\eqref{eq17}} \tag{18} \label{eq18}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly for layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{w_{ji}^l}} &amp;=\delta_j^l \cdot h_i^{l-1} \tag{19} \label{eq19} \\
\delta_j^l &amp;= \frac{\partial{J}}{\partial{z_j^l}} \\
&amp;= \sum_{k=1}^{r} \frac{\partial{J}}{\partial{z_k^o}} \cdot  \frac{\partial{z_k^o}}{\partial{z_j^l}}\\
&amp;= \sum_{k=1}^{r} \frac{\partial{J}}{\partial{z_k^o}} \cdot  \frac{\partial}{\partial{z_j^l}} \big( \sum_{j=1}^{q} w_{kj}^o  h_j^{l} + b_k^o \big) &amp;&amp; \text{by \eqref{eq5}}\\
&amp;= \sum_{k=1}^{r} \frac{\partial{J}}{\partial{z_k^o}} \cdot w_{kj}^o \cdot  \frac{\partial{h_j^l}}{\partial{z_j^l}} \\
\delta_j^l &amp;= \sum_{k=1}^{r} \delta_k^o \cdot w_{kj}^o \cdot \sigma'({z_j^l}) &amp;&amp; \text{by \eqref{eq11},\eqref{eq16}}\\
\delta_j^l &amp;= \sum_{k=1}^{r} \delta_k^{l+1} \cdot w_{kj}^{l+1} \cdot \sigma'({z_j^l}) &amp;&amp; \text{by,  } {o=l+1} \tag{20} \label{eq20}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly you can easily prove for bias,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{b_j^l}} &amp;=\delta_j^l \tag{21} \label{eq21}\\
\frac{\partial{J}}{\partial{b_k^o}} &amp;=\delta_k^o \tag{22} \label{eq22}\\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h6 id=&quot;backword-pass-vectorized&quot;&gt;Backword pass (Vectorized)&lt;/h6&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
\delta^o &amp;= \nabla_{y'}J \odot \sigma'(z^o) &amp;&amp; \text{by \eqref{eq17}} \tag{23} \label{eq23}\\
\delta^l &amp;= ((W^{l+1})^T \otimes \delta^{l+1} )\odot  \sigma'({z^l}) &amp;&amp; \text{by \eqref{eq20}} \tag{24} \label{eq24}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;(W^l)^T&lt;/script&gt; is transpose of matrix &lt;script type=&quot;math/tex&quot;&gt;W^l&lt;/script&gt;  and &lt;script type=&quot;math/tex&quot;&gt;\nabla_{y'}J&lt;/script&gt;, Derivative of J with respect recpect to vector &lt;script type=&quot;math/tex&quot;&gt;y'&lt;/script&gt;, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align} 
\nabla_{y'}J =  \frac{\partial{J}}{\partial{y'}}= \left[\begin{array}{c} \frac{\partial{J}}{y'_1} \\  \frac{\partial{J}}{y'_2} \\ \vdots \\ \frac{\partial{J}}{y'_k} \\ \vdots \end{array}\right]
\end{align}&lt;/script&gt;

&lt;p&gt;Also,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\nabla_{b^l}J &amp;=  \delta^l   &amp;&amp; \text{by \eqref{eq21}} \tag{25} \label{eq25}\\
\nabla_{W^l}J &amp;=  \delta^l  \otimes (h^{l-1})^T &amp;&amp; \text{by \eqref{eq19}} \tag{26} \label{eq26}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Where, &lt;script type=&quot;math/tex&quot;&gt;\nabla_{W^l}J&lt;/script&gt;, Derivative of J with respect recpect to matrix &lt;script type=&quot;math/tex&quot;&gt;W^l&lt;/script&gt;, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\nabla_{W^l}J =  \frac{\partial{J}}{\partial{W^l}}  = \begin{bmatrix} \frac{\partial{J}}{\partial{w_{11}}} &amp; \frac{\partial{J}}{\partial{w_{12}}} &amp; \cdots &amp; \frac{\partial{J}}{\partial{w_{1i}}} &amp; \cdots \\ \frac{\partial{J}}{\partial{w_{21}}} &amp; \frac{\partial{J}}{\partial{w_{22}}} &amp; \cdots &amp; \frac{\partial{J}}{\partial{w_{2i}}} &amp; \cdots \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ \frac{\partial{J}}{\partial{w_{j1}}} &amp; \frac{\partial{J}}{\partial{w_{j2}}} &amp; \cdots &amp; \frac{\partial{J}}{\partial{w_{ji}}} &amp; \cdots \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \end{bmatrix}
\end{align} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;update-step&quot;&gt;Update step:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Given a training batch of size &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; and learning rate &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;For each training example in &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; in batch do &lt;strong&gt;Forward pass&lt;/strong&gt; and Backward pass, accumulate &lt;script type=&quot;math/tex&quot;&gt;\nabla_{b^l}J^{(i)}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\nabla_{W^l}J^{(i)}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Update weights and bias for each layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; as follows,&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
b_l &amp;= b_l -\eta \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} \nabla_{b^l}J^{(i)} \tag{27} \label{eq27}\\
W_l &amp;= W_l - \eta \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} \nabla_{W^l}J^{(i)} \tag{28} \label{eq28}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;tddr&quot;&gt;td;dr.&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
h^l &amp;= \sigma(z^l) &amp;&amp; \text{by \eqref{eq7}} \\
z^l &amp;= W^l \otimes h^{l-1} + b^l &amp;&amp; \text{by \eqref{eq8}}  \\
\delta^o &amp;= \nabla_{y'}J \odot \sigma'(z^o) &amp;&amp; \text{by \eqref{eq23}} \\
\delta^l &amp;= ((W^{l+1})^T \otimes \delta^{l+1} )\odot  \sigma'({z^l}) &amp;&amp; \text{by \eqref{eq24}} \\
\nabla_{b^l}J &amp;=  \delta^l   &amp;&amp; \text{by \eqref{eq26}} \\
\nabla_{W^l}J &amp;=  \delta^l  \otimes (h^{l-1})^T &amp;&amp; \text{by \eqref{eq26}} \\
b_l &amp;= b_l -\eta \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} \nabla_{b^l}J^{(i)} &amp;&amp; \text{by \eqref{eq27}}  \\
W_l &amp;= W_l - \eta \cdot \frac{1}{m} \cdot \sum_{i=1}^{m} \nabla_{W^l}J^{(i)} &amp;&amp; \text{by \eqref{eq28}} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;code&quot;&gt;&lt;a href=&quot;https://github.com/rakesh-malviya/MLCodeGems/tree/master/Projects/Neural_networks/src&quot;&gt;Code&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/rakesh-malviya/MLCodeGems/tree/master/Projects/Neural_networks/src&quot;&gt;Here&lt;/a&gt; is the python implementation of the above article.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Neural Networks and Deep Learning Chapter 2 &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap2.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Sun, 17 Sep 2017 00:00:00 +0530</pubDate>
				<link>http://localhost:4000/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html</link>
				<guid isPermaLink="true">http://localhost:4000/2017/09/17/4-neural-networks-part-3-feedforward-neural-network.html</guid>
			</item>
		
			<item>
				<title>3. Neural Networks Part 2: Activation functions and differentiation</title>
        <description>&lt;h2 id=&quot;activation-functions&quot;&gt;Activation functions&lt;/h2&gt;

&lt;p&gt;A neural network is a network of artificial neurons connected to each other in a specific way. Job of neural network is to learn from given data. The prediction function that neural network must learn can be non-linear. Activation function in artificial neurons helps the neural network to learn non-linear prediction function.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Linear prediction&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Non-linear prediction&lt;sup&gt;&lt;a href=&quot;#references&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/3/3_1_linear.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/img/blog/3/3_2_nonlinear.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Activation functions (generally) have functional form of &lt;script type=&quot;math/tex&quot;&gt;f(u) = f(w^{T}{x} + b)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;u = {b} + \sum_{j=1}^{n} {w_j}\cdot{x_j} = w^{T}{x} + b&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;w = (w_1,w_2,.......w_n)&lt;/script&gt; weight vector  and &lt;script type=&quot;math/tex&quot;&gt;x= (x_1,x_2,.....x_n)&lt;/script&gt; single training data vector&lt;/p&gt;

&lt;h4 id=&quot;1-sigmoid-activation-function&quot;&gt;1. Sigmoid activation function&lt;/h4&gt;

&lt;p&gt;A sigmoid function, &lt;script type=&quot;math/tex&quot;&gt;f(u) = \frac{1}{1+e^{-u}}&lt;/script&gt;.  It takes a real-valued number and ‚Äúsqueeze‚Äù it into range between 0 and 1. Large negative numbers become &lt;script type=&quot;math/tex&quot;&gt;\approx 0&lt;/script&gt; and large positive numbers become &lt;script type=&quot;math/tex&quot;&gt;\approx 1&lt;/script&gt;.&lt;/p&gt;

&lt;h5 id=&quot;pros&quot;&gt;Pros:&lt;/h5&gt;
&lt;p&gt;For classification problem it is used as activation of output layer of a neural network.&lt;/p&gt;

&lt;h5 id=&quot;cons&quot;&gt;Cons:&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Saturate and kill gradients:&lt;/strong&gt; When neuron‚Äôs activation saturates at 1 or 0 , the gradient becomes almost zero. Which will make the neuron unable to backpropagate and learn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Outputs are not zero-centered:&lt;/strong&gt; Since outputs are in range 0 to 1 neurons in next layer will receive data that is not zero centered. Hence gradient of weights &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; during backpropagation will be either all positive or all negative, which can cause undesirable zig-zagging dynamics in gradient updates of weights. When considering gradient added over all training data in a batch this problem is not much severe compared to ‚ÄúSaturate and kill gradients‚Äù&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;2-tanh-activation-function&quot;&gt;2. Tanh activation function&lt;/h4&gt;
&lt;p&gt;A tanh function, &lt;script type=&quot;math/tex&quot;&gt;f(u) = \frac{e^{u}-e^{-u}}{e^{u}+e^{-u}} = \frac{sinh(u)}{cosh(u)}&lt;/script&gt;.  It takes a real-valued number and ‚Äúsqueeze‚Äù it into range between -1 and 1. Large negative numbers become &lt;script type=&quot;math/tex&quot;&gt;\approx -1&lt;/script&gt; and large positive numbers become &lt;script type=&quot;math/tex&quot;&gt;\approx 1&lt;/script&gt;.&lt;/p&gt;

&lt;h5 id=&quot;pros-1&quot;&gt;Pros:&lt;/h5&gt;
&lt;p&gt;It is preferred over sigmoid because its outputs are zero centered&lt;/p&gt;

&lt;h4 id=&quot;3-relu-activation-function&quot;&gt;3. ReLU activation function&lt;/h4&gt;
&lt;p&gt;The Rectified Linear Unit, ReLU is &lt;script type=&quot;math/tex&quot;&gt;f(u) = max(0,u)&lt;/script&gt;&lt;/p&gt;

&lt;h5 id=&quot;pros-2&quot;&gt;Pros:&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Greatly increase training speed compared to tanh and sigmoid&lt;/li&gt;
  &lt;li&gt;Less expensive computations compared to tanh and sigmoid&lt;/li&gt;
  &lt;li&gt;Reduces likelihood of the gradient to vanish. Since when &lt;script type=&quot;math/tex&quot;&gt;u &gt; 0&lt;/script&gt;, the gradient has constant value.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sparsity:&lt;/strong&gt; When more &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
u &lt;= 0 %]]&gt;&lt;/script&gt;, the &lt;script type=&quot;math/tex&quot;&gt;f(u)&lt;/script&gt; can be more sparse&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;cons-1&quot;&gt;Cons:&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Tend to blow up activation (there is no mechanism to constrain the output of the neuron, as &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; itself is the output).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Closed ReLU or Dead ReLU&lt;/strong&gt;: If inputs tend to make &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
u&lt;=0 %]]&gt;&lt;/script&gt; than the most of the neurons will always have 0 gradient updates hence closed or dead.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;4-leaky-relu&quot;&gt;4. Leaky ReLU:&lt;/h4&gt;

&lt;p&gt;It solves the dead ReLU problem. &lt;script type=&quot;math/tex&quot;&gt;0.01&lt;/script&gt; is coefficient of leakage.  Leaky ReLU is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(u)= 
\begin{cases}
    x,&amp; \text{if } x &gt; 0\\
    (0.01)x,              &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;5-parameterized-relu-or-prelu&quot;&gt;5. Parameterized ReLU or PReLU:&lt;/h4&gt;
&lt;p&gt;Parameterizes coefficient of leakage &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; in Leaky ReLU.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
f(u)= 
\begin{cases}
    x,&amp; \text{if } x &gt; 0\\
    \alpha{x},              &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;6-maxout&quot;&gt;6. Maxout&lt;/h4&gt;
&lt;p&gt;Generalization of ReLU, Leaky ReLU and PReLU. It does not have functional form of &lt;script type=&quot;math/tex&quot;&gt;f(u) =  f(w^{T}{x} + b)&lt;/script&gt;, instead it computes function &lt;script type=&quot;math/tex&quot;&gt;max({w'^T}{x} + b',{w^T}{x} + b)&lt;/script&gt;.&lt;/p&gt;

&lt;h5 id=&quot;pros-3&quot;&gt;Pros:&lt;/h5&gt;
&lt;p&gt;Maxout has pros of ReLU but doesn‚Äôt have dead ReLU issue&lt;/p&gt;

&lt;h5 id=&quot;cons-2&quot;&gt;Cons:&lt;/h5&gt;
&lt;p&gt;It has twice number of weight parameters to learn &lt;script type=&quot;math/tex&quot;&gt;w'&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-activation-function-should-i-use-2&quot;&gt;What Activation function should I use ?&lt;sup&gt;&lt;a href=&quot;#references&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;For output layer use sigmoid if classification task&lt;/li&gt;
  &lt;li&gt;For output layer use no activation or Purelin function &lt;script type=&quot;math/tex&quot;&gt;f(u) = u&lt;/script&gt; if regression task&lt;/li&gt;
  &lt;li&gt;For other neurons:
    &lt;ol&gt;
      &lt;li&gt;Use the ReLU non-linearity if you carefully set learning rates and monitor the fraction of ‚Äúdead ReLU‚Äù in network.&lt;/li&gt;
      &lt;li&gt;Else try Leaky ReLU or Maxout.&lt;/li&gt;
      &lt;li&gt;Or try tanh but it will work worse than ReLU&lt;/li&gt;
      &lt;li&gt;Never use sigmoid&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;differentiation&quot;&gt;Differentiation:&lt;/h2&gt;
&lt;h4 id=&quot;basic-formulas&quot;&gt;Basic formulas:&lt;/h4&gt;
&lt;p&gt;Given &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;g(x)&lt;/script&gt; are differentiable functions (the derivative exists), &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; are any real numbers:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{d}{dx}f(x) &amp;= f'(x) \tag{1} \label{eq1} \\
\frac{d}{dx}g(x) &amp;= g'(x) \tag{2} \label{eq2} \\
\frac{d}{dx}(f(x) \pm g(x)) &amp;= \frac{d}{dx}f(x) \pm \frac{d}{dx}g(x)  \\
 &amp;=  f'(x) \pm g'(x) \tag{3}  \label{eq3} \\
\frac{d}{dx}x^n &amp;= nx^{n-1}  &amp;&amp; \text{power-rule} \tag{4} \label{eq4} \\
\frac{d}{dx} f(x)g(x) &amp;= f'(x)g(x) + f(x)g'(x) &amp;&amp; \text{product-rule} \tag{5} \label{eq5} \\ 
\frac{d}{dx} \Bigg [\frac{f(x)}{g(x)}\Bigg ]  &amp;= \frac{f'(x)g(x)-g'(x)f(x)}{g^{2}(x)} &amp;&amp; \text{Quotient Rule} \tag{6} \label{eq6} \\ 
\frac{d}{dx} f(g(x))  &amp;= f'(g(x))g'(x) &amp;&amp; \text{Chain Rule} \tag{7} \label{eq7} \\
\frac{d}{dx} c &amp;= 0 \tag{8} \label{eq8} \\
tanh(x) &amp;= \frac{sinh(x)}{cosh(x)}  \tag{9} \label{eq9} \\
\frac{d}{dx}sinh(x) &amp;= cosh(x)  \tag{10} \label{eq10} \\
\frac{d}{dx}cosh(x) &amp;= sinh(x)  \tag{11} \label{eq11} \\
\frac{d}{dx}e^x &amp;= e^x  \tag{12} \label{eq12} \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;sigmoid-function&quot;&gt;Sigmoid function:&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{d}{dx} f(x) &amp;= \frac{d}{dx} \Bigg[ \frac{1}{1+e^{-x}} \Bigg] \\
&amp;= \frac{d}{dx} \bigg[ \frac{e^x}{1+e^x} \bigg] \\ 
&amp;= \frac {\bigg(\frac{d}{dx}e^x\bigg)(1+e^x) - \bigg(\frac{d}{dx}(1 + e^x)\bigg)(e^x)}{(1+e^x)^2} &amp;&amp;  \text{by \eqref{eq6}} \\
&amp;= \frac {\bigg(\frac{d}{dx}e^x\bigg)(1+e^x) - \bigg(\frac{d}{dx} 1 +\frac{d}{dx} e^x\bigg)(e^x)}{(1+e^x)^2} &amp;&amp; \text{by \eqref{eq3}} \\
&amp;= \frac {(e^x)(1+e^x) - (e^x)(e^x)}{(1+e^x)^2} &amp;&amp; \text{by \eqref{eq8} and \eqref{eq12}} \\
&amp;= \bigg[\frac {(e^x)}{(1+e^x)}\bigg]  - \bigg[\frac{(e^x)^2}{(1+e^x)^2}\bigg] \\
&amp;= \bigg[\frac {e^x}{1+e^x}\bigg]  - \bigg[\frac{e^x}{1+e^x}\bigg]^2 \\
&amp;= \bigg[\frac {1}{1+e^{-x}}\bigg]  - \bigg[\frac{1}{1+e^{-x}}\bigg]^2 \\
&amp; = f(x) - (f(x))^2 \\
&amp; = f(x) (1 - f(x)) \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;tanh-function&quot;&gt;Tanh function:&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{d}{dx} f(x) &amp;= \frac{d}{dx} \Bigg[ \frac{sinh(x)}{cosh(x)} \Bigg] \\
&amp;= \Bigg[ \frac{\bigg(\frac{d}{dx}sinh(x)\bigg)cosh(x) - \bigg(\frac{d}{dx}cosh(x)\bigg)sinh(x)}{(cosh(x))^2} \Bigg] &amp;&amp; \text{by \eqref{eq6}} \\
&amp; = \frac{(cosh(x))^2 - (sinh(x))^2}{(cosh(x))^2} &amp;&amp; \text{by \eqref{eq10} and \eqref{eq11}} \\ 
&amp; = 1 - \bigg(\frac{sinh(x)}{cosh(x)}\bigg)^2 \\
&amp; = 1 - (tanh(x))^2 &amp;&amp; \text{by \eqref{eq9}} \\
&amp; = 1 - (f(x))^2 \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Tensorflow playground &lt;a href=&quot;http://playground.tensorflow.org&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;http://cs231n.github.io&lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Sat, 02 Sep 2017 00:00:00 +0530</pubDate>
				<link>http://localhost:4000/2017/09/02/3-neural-networks-part-2-activation-functions-and-differentiation.html</link>
				<guid isPermaLink="true">http://localhost:4000/2017/09/02/3-neural-networks-part-2-activation-functions-and-differentiation.html</guid>
			</item>
		
			<item>
				<title>2. Neural Networks Part 1: Logistic Regression (Least Square Error)</title>
        <description>&lt;p&gt;Required Learning: Linear regression basics &lt;a href=&quot;http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are starting from basic unit of Neural networks single activation neuron. A Neural network with single neuron is same as logistic regression. Therefore a neural network can be considered as a networked set of logistic regression units.&lt;/p&gt;

&lt;h4 id=&quot;establish-notations-for-future-use1&quot;&gt;Establish notations for future use&lt;sup&gt;&lt;a href=&quot;#references&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; to denote the i&lt;sup&gt;th&lt;/sup&gt; ‚Äúinput ‚Äù of training data&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; to denote the i&lt;sup&gt;th&lt;/sup&gt; ‚Äúoutput‚Äù or target of training data&lt;/li&gt;
  &lt;li&gt;Pair &lt;script type=&quot;math/tex&quot;&gt;(x^{(i)}, y^{(i)})&lt;/script&gt; is called a training example&lt;/li&gt;
  &lt;li&gt;The dataset that we‚Äôll be using to learn‚Äîa list of m training examples &lt;script type=&quot;math/tex&quot;&gt;\{(x(i), y(i)); i = 1, . . . , m\}&lt;/script&gt; ‚Äî is called a training set&lt;/li&gt;
  &lt;li&gt;Each &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; in training set can have &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; &lt;strong&gt;features&lt;/strong&gt; such that &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; is a vector &lt;script type=&quot;math/tex&quot;&gt;(x^{(i)}_1,x^{(i)}_2,x^{(i)}_3,..... x^{(i)}_n)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;In current setup of logistic regression &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; is scalar value&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note that the superscript ‚Äú(i)‚Äù in the notation is simply an index into the training set, and has nothing to do with exponentiation.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blog/2/2_1_logistic_reg.svg&quot; alt=&quot;&quot; title=&quot;Single neuron&quot; /&gt;
&lt;i&gt;&lt;center&gt;Fig: Single neuron&lt;/center&gt;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;y'^{(i)} = f(u^{(i)})&lt;/script&gt; , 
where&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;y'^{(i)}&lt;/script&gt; is predicted output&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is activation function&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;u^{(i)} = {b} + \sum_{j=1}^{n} {w_j}\cdot{x_j^{(i)}}&lt;/script&gt;, for &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; training example, where &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is bias of the neuron.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt; &lt;strong&gt;weights&lt;/strong&gt; or &lt;strong&gt;training parameters&lt;/strong&gt; we need to learn&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will apply gradient descent to minimize the squared error cost function &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt;, also called least square error&lt;/p&gt;

&lt;p&gt;\begin{align}
J = \frac{1}{2m}\sum_{i=1}^{m} (y^{(i)} - y‚Äô^{(i)})^2 \tag{1} \label{eq1}
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; term &lt;script type=&quot;math/tex&quot;&gt;2&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2m}&lt;/script&gt; makes the derivative of J much simpler as you will see later. With &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2m}&lt;/script&gt; loss function value &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; does not depend on the size of training data i.e. &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; which makes it easy for comparison for different values of &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; or batch size in case of &lt;strong&gt;mini-batch stochatic gradient&lt;/strong&gt; that you will see later sections.&lt;/p&gt;

&lt;p&gt;We will use Sigmoid function as activation function, i.e. &lt;script type=&quot;math/tex&quot;&gt;\sigma(u)&lt;/script&gt;
\begin{align}
\sigma(u^{(i)}) = \dfrac{1}{1+e^{-u^{(i)}}} = f(u^{(i)}) = y‚Äô^{(i)}   \tag{2} \label{eq2}
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;derivatives&quot;&gt;Derivatives&lt;/h4&gt;

&lt;p&gt;\begin{align}
\frac{\partial\sigma(u)}{\partial{u}} = \sigma(u)\cdot(1 - \sigma(u))   \tag{3} \label{eq3}
\end{align}
\begin{align}
\frac{\partial{J}}{\partial{y‚Äô^{(i)}}} = y‚Äô^{(i)} - y^{(i)}   \tag{4} \label{eq4}
\end{align}
\begin{align}
\frac{\partial{u^{(i)}}}{\partial{w_j}} = x_j^{(i)}   \tag{5} \label{eq5}
\end{align}
\begin{align}
\frac{\partial{u^{(i)}}}{\partial{b}} = 1   \tag{6} \label{eq6}
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h4&gt;

&lt;p&gt;To learn &lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; parameter so that &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; is minimum. We will find &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial{J}}{\partial{w_j}}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial{J}}{\partial{b}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note: &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; is our loss function and &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; is for indexing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since, &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; is function of &lt;script type=&quot;math/tex&quot;&gt;y'^{(i)}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y'^{(i)}&lt;/script&gt; is function of &lt;script type=&quot;math/tex&quot;&gt;u^{(i)}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;u^{(i)}&lt;/script&gt; is function of &lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\eqref{eq1} and \eqref{eq2}&lt;/script&gt;. Using chain rule of differentiation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{w_j}} &amp;= \frac{1}{m}\sum_{i=1}^{m}  \frac{\partial{J}}{\partial{y'^{(i)}}} \cdot  \frac{\partial{y'^{(i)}}}{\partial{w_j}}   &amp;&amp; \text{by \eqref{eq1}} \\
&amp;= \frac{1}{m}\sum_{i=1}^{m}  \frac{\partial{J}}{\partial{y'^{(i)}}} \cdot  \frac{\partial{y'^{(i)}}}{\partial{u^{(i)}}} \cdot    \frac{\partial{u^{(i)}}}{\partial{w_j}}  \\
&amp;= \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  \frac{\partial{y'^{(i)}}}{\partial{u^{(i)}}} \cdot    \frac{\partial{u^{(i)}}}{\partial{w_j}}  &amp;&amp; \text{by \eqref{eq4}} \\
&amp;= \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot    \frac{\partial{u^{(i)}}}{\partial{w_j}}  &amp;&amp; \text{by \eqref{eq2} and \eqref{eq3}} \\
\frac{\partial{J}}{\partial{w_j}} &amp;= \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot  x_j^{(i)} &amp;&amp; \text{by \eqref{eq5}} \tag{7} \label{eq7}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Sum (&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^{m}&lt;/script&gt;) and averaging (&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{m}&lt;/script&gt;)of gradient is needed for following reasons:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Summing of individual gradients on training examples makes gradient update smoother&lt;/li&gt;
  &lt;li&gt;Without averaging the learning rate depends on the size of training data &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; or batch size&lt;/li&gt;
  &lt;li&gt;With averaging the gradient magnitude is independent of the batch size. This allows comparison when using different batch sizes or training data size &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So during the training update equation for &lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt; over all &lt;script type=&quot;math/tex&quot;&gt;j = 1 ..... n&lt;/script&gt; is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
w_j &amp;= w_j - \eta \cdot \frac{\partial{J}}{\partial{w_j}} \\
&amp;= w_j - \eta \cdot  \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot  x_j^{(i)} &amp;&amp; \text{by \eqref{eq7}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
\frac{\partial{J}}{\partial{b}} &amp;=  \frac{1}{m}\sum_{i=1}^{m}  \frac{\partial{J}}{\partial{y'^{(i)}}} \cdot  \frac{\partial{y'^{(i)}}}{\partial{b}}   &amp;&amp; \text{by \eqref{eq1}} \\
&amp;=  \frac{1}{m}\sum_{i=1}^{m}  \frac{\partial{J}}{\partial{y'^{(i)}}} \cdot  \frac{\partial{y'^{(i)}}}{\partial{u^{(i)}}} \cdot    \frac{\partial{u^{(i)}}}{\partial{b}}  \\
&amp;=  \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  \frac{\partial{y'^{(i)}}}{\partial{u^{(i)}}} \cdot    \frac{\partial{u^{(i)}}}{\partial{b}}  &amp;&amp; \text{by \eqref{eq4}} \\
&amp;=  \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot    \frac{\partial{u^{(i)}}}{\partial{b}}  &amp;&amp; \text{by \eqref{eq2} and \eqref{eq3}} \\
\frac{\partial{J}}{\partial{b}} &amp;=  \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) &amp;&amp; \text{by \eqref{eq6}} \tag{8} \label{eq8}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Update equation for bias &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
b &amp;= b - \eta \cdot \frac{\partial{J}}{\partial{b}} \\
&amp;= b - \eta \cdot  \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) &amp;&amp; \text{by \eqref{eq8}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Training Steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Choose an initial vector of parameters  &lt;script type=&quot;math/tex&quot;&gt;w = (w_1,......w_n)&lt;/script&gt;, bias &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; and learning rate &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Repeat for predifined epoch such that approximate minimum &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; loss is obtained:
    &lt;ol&gt;
      &lt;li&gt;Evaluate and store &lt;script type=&quot;math/tex&quot;&gt;y'^{(i)}&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;i = 1,2,3...m&lt;/script&gt; training examples by using equation &lt;script type=&quot;math/tex&quot;&gt;\eqref{eq2}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;Update bias, &lt;script type=&quot;math/tex&quot;&gt;b = b - \eta \cdot  \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)})&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;For &lt;script type=&quot;math/tex&quot;&gt;j = 1,2,.....n&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; :
        &lt;ol&gt;
          &lt;li&gt;Update, &lt;script type=&quot;math/tex&quot;&gt;w_j = w_j - \eta \cdot  \frac{1}{m}\sum_{i=1}^{m}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot  x_j^{(i)}&lt;/script&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Code snippet of above steps:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    
    #Accumulate gradient with respect to bias and weights
    grad_bias = 0
    grad_w = np.zeros(len(W))
    for i in range(X_train.shape[0]):        
        grad_bias += (YP[i] - y_train[i])*(YP[i])*(1-YP[i]) #dJ/db
        for j in range(len(W)):
            #dJ/dW_j
            grad_w[j] += (YP[i] - y_train[i])*(YP[i])*(1-YP[i])*(X_train[i][j])
        
    #Update bias
    bias = bias - grad_bias*lr/X_train.shape[0]    
    
    #Update weights    
    for j in range(len(W)):
        W[j] = W[j] - grad_w[j]*lr/X_train.shape[0]

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;code&quot;&gt;&lt;a href=&quot;https://github.com/rakesh-malviya/MLCodeGems/blob/master/notebooks/Neural_networks/3-neural-networks-part-1-logistic-regression-least-square-error.ipynb&quot;&gt;Code&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/rakesh-malviya/MLCodeGems/blob/master/notebooks/Neural_networks/3-neural-networks-part-1-logistic-regression-least-square-error.ipynb&quot;&gt;Here&lt;/a&gt; is the python implementation of the above article.&lt;/p&gt;

&lt;h4 id=&quot;stochastic-gradient-descent-sgd&quot;&gt;Stochastic gradient descent, SGD&lt;/h4&gt;
&lt;p&gt;When training data size &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is large we choose &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
m' &lt; m %]]&gt;&lt;/script&gt;  of batch size. We divide our training data into batches of size &lt;script type=&quot;math/tex&quot;&gt;m'&lt;/script&gt;. We update weights and bias for each batch as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Choose an initial vector of parameters  &lt;script type=&quot;math/tex&quot;&gt;w = (w_1,......w_n)&lt;/script&gt;, bias &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; and learning rate &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Divide training data into batches of size &lt;script type=&quot;math/tex&quot;&gt;m'&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Repeat for predifined epoch such that approximate minimum &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; loss is obtained:
    &lt;ol&gt;
      &lt;li&gt;Repeat for each batch:
        &lt;ol&gt;
          &lt;li&gt;Evaluate and store &lt;script type=&quot;math/tex&quot;&gt;y'^{(i)}&lt;/script&gt; for all &lt;script type=&quot;math/tex&quot;&gt;i = 1,2,3...m&lt;/script&gt; training examples by using equation &lt;script type=&quot;math/tex&quot;&gt;\eqref{eq2}&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;Update bias, &lt;script type=&quot;math/tex&quot;&gt;b = b - \eta \cdot  \frac{1}{m'}\sum_{i=1}^{m'}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)})&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;For &lt;script type=&quot;math/tex&quot;&gt;j = 1,2,.....n&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; : Update, &lt;script type=&quot;math/tex&quot;&gt;w_j = w_j - \eta \cdot  \frac{1}{m'}\sum_{i=1}^{m'}  (y'^{(i)} - y^{(i)}) \cdot  y'^{(i)} \cdot (1 - y'^{(i)}) \cdot  x_j^{(i)}&lt;/script&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;advantages-of-sgd&quot;&gt;Advantages of SGD&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Much faster than normal gradient descent&lt;/li&gt;
  &lt;li&gt;Better choice when whole training data cannot fit into the RAM (available memory) of the system&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;http://cs229.stanford.edu/notes&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;previous-posts&quot;&gt;Previous Posts:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;/2017/08/15/word2vec-basics.html&quot;&gt;‚Äú1. Word2vec Part 1: Basics‚Äù&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 25 Aug 2017 00:00:00 +0530</pubDate>
				<link>http://localhost:4000/2017/08/25/2-neural-networks-part-1-logistic-regression-least-square-error.html</link>
				<guid isPermaLink="true">http://localhost:4000/2017/08/25/2-neural-networks-part-1-logistic-regression-least-square-error.html</guid>
			</item>
		
			<item>
				<title>1. Word2vec Part 1: Basics</title>
        <description>&lt;p&gt;For NLP with Deep learning there is need to represent text into data that can be understood by Neural networks.&lt;/p&gt;

&lt;h2 id=&quot;one-hot-encoding&quot;&gt;One Hot encoding&lt;/h2&gt;

&lt;p&gt;In this encoding we represent word vectors by zeros and ones.&lt;/p&gt;

&lt;p&gt;For e.g.  assume we have following text (or corpus): &lt;strong&gt;‚ÄúI like Deep learning and NLP‚Äù&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Than our vocabulary is as follows:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Word&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Word&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Index&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;¬†&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;I&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;like&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Deep&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;learning&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;and&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;NLP&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Than &lt;strong&gt;one-hot representation&lt;/strong&gt; of each word will be as follows:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Word&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;One-hot&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;I&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;like&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;010000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Deep&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;001000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;learning&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;000100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;and&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;000010&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;NLP&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;000001&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Notice the position of word in vocabulary and position of the bit set in its one hot representation&lt;/p&gt;

&lt;p&gt;Under this representation, each word is Independent. It hard find its relationship with other words in Corpus. Because of this one hot is also called &lt;strong&gt;Local representation&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;word2vec-intuition&quot;&gt;Word2Vec Intuition&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;‚ÄúYou shall know a word by the company it keeps‚Äù - By J.R. Firth 1957‚Äù&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can get lot of value by representing a word by means of its neighbors. This way of representing a word given the distributions of its neighbors is called &lt;strong&gt;Distributional similarity based representation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Word2Vec is also called &lt;strong&gt;Distributed representation&lt;/strong&gt; because of how it is different from ‚ÄúOne Hot‚Äù representation which is local.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;government debt problems turning into&lt;/strong&gt; banking &lt;strong&gt;crises as has happened in&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;saying that Europe needs unified&lt;/strong&gt; banking &lt;strong&gt;regulation to replace the hodgepodge&lt;/strong&gt;&lt;br /&gt;
In above two sentences words in &lt;strong&gt;bold&lt;/strong&gt; describe the word ‚Äúbanking‚Äù&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Warning Note:&lt;/strong&gt; Below example gives only a partial picture of how word2vec algorithm can understands similarity between words but this is not how this algorithm is implemented:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Let us assume we have the following sentences in our text.

1. Sam is **good** boy
2. Sam is **fine** boy
3. Sam is **great** boy

Given the neighbor words &quot;Sam&quot;, &quot;is&quot;, and &quot;boy&quot; algorithms understands that &quot;good&quot;, &quot;fine&quot;, and &quot;great&quot; are similar words
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Word2Vec algorithm can be implemented in 2 ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Skip Gram model&lt;/li&gt;
  &lt;li&gt;CBOW (Continuous Bag-of-word) Model&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;skip-gram-model&quot;&gt;Skip Gram model&lt;/h2&gt;

&lt;p&gt;Let us assume we have following text (or corpus): &lt;strong&gt;‚ÄúI like Deep learning and NLP‚Äù&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Word vec&lt;/th&gt;
      &lt;th&gt;Word&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;I&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;like&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;deep&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;w_3&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;learning&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;w_4&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;and&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;w_5&lt;/script&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;NLP&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Skip gram defines a model that predicts context words given center word &lt;script type=&quot;math/tex&quot;&gt;w_t&lt;/script&gt;. So the skip gram model trains to maximize probability of context word (neighbor words) given center words. &lt;br /&gt;
i.e. &lt;script type=&quot;math/tex&quot;&gt;P(w_{c}  \vert  w_t)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;For  t = 0&lt;br /&gt;
center word, &lt;script type=&quot;math/tex&quot;&gt;w_t&lt;/script&gt; =&lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; = (‚ÄúI‚Äù)&lt;br /&gt;
let window for neighbors is 5 so,&lt;br /&gt;
context words = &lt;script type=&quot;math/tex&quot;&gt;w_{c}&lt;/script&gt; = (‚Äúlike‚Äù,‚Äùdeep‚Äù)&lt;/p&gt;

&lt;p&gt;For t= 2:&lt;br /&gt;
center word, &lt;script type=&quot;math/tex&quot;&gt;w_t&lt;/script&gt; = &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt; = (‚Äúdeep‚Äù)&lt;br /&gt;
let window for neighbors is 5 so,&lt;br /&gt;
context words = &lt;script type=&quot;math/tex&quot;&gt;w_{c}&lt;/script&gt; = (‚ÄúI‚Äù, ‚Äúlike‚Äù, ‚Äúlearning‚Äù, ‚Äúand‚Äù)&lt;/p&gt;

&lt;p&gt;So we can start by initialize random words vectors than adjust there values while training to maximize probability  &lt;script type=&quot;math/tex&quot;&gt;P(w_c \vert w_t)&lt;/script&gt; or minimize loss function &lt;script type=&quot;math/tex&quot;&gt;J =1- P(w_c \vert w_t)&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;algorithm-steps&quot;&gt;Algorithm steps:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Look at different positions of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Calculate loss function &lt;script type=&quot;math/tex&quot;&gt;J =1- P(w_c \vert w_t)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Adjust values of word vectors &lt;script type=&quot;math/tex&quot;&gt;w_t&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;cbow-continuous-bag-of-word-model&quot;&gt;CBOW (Continuous Bag-of-word) Model&lt;/h2&gt;
&lt;p&gt;CBOW defines a model that predicts center word &lt;script type=&quot;math/tex&quot;&gt;w_t&lt;/script&gt; given context words. So CBOW model trains to maximizeprobability of context word (neighbor words) given center words. &lt;br /&gt;
i.e. &lt;script type=&quot;math/tex&quot;&gt;P(w_t \vert w_c)&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;For us humans it is very easy to predict fill in the blanks like below:
The Leopard ____ very fast
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;next-&quot;&gt;Next ?&lt;/h2&gt;
&lt;p&gt;In next posts we will dive deep into word2vec derivations and algorithms&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Mikolov, Tomas, et al. ‚ÄúDistributed representations of words and phrases and their compositionality.‚Äù Advances in neural information processing systems. 2013.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stanford CS224n: Natural Language Processing with Deep Learning&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Tue, 15 Aug 2017 00:00:00 +0530</pubDate>
				<link>http://localhost:4000/2017/08/15/word2vec-basics.html</link>
				<guid isPermaLink="true">http://localhost:4000/2017/08/15/word2vec-basics.html</guid>
			</item>
		
	</channel>
</rss>
